{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_1st_chapter2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YHI3vyhv5p85"
      },
      "source": [
        "## ঝিঁঝিঁ পোকার থার্মোমিটার \n",
        "\n",
        "Rev 4.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F8YVA_634OFk"
      },
      "source": [
        "মনে আছে ছোট্ট রকিব এবং তার ফুপিমার গল্পের কথা? আমি আগের বই “শূন্য থেকে পাইথন মেশিন লার্নিং” বইটার কথা বলছিলাম। সেখানে গল্পটা ছিল এরকম, কোন এক রাত্রে যখন সবাই গরমে কাহিল, তখন ওই সময়ে কত টেম্পারেচার সেটা নিয়ে কথা হচ্ছিল ছোট্ট রকিবের গ্রামের দাদা বাড়িতে। ফুঁপিমা বলছিলেন, বাইরের বসার ঘরের সেই বড় থার্মোমিটার না দেখেও তখনকার টেম্পারেচার আন্দাজ করা যাবে ঝিঁঝিঁপোকার ডাক থেকে। সবাই অবাক, সবার প্রশ্ন কিভাবে?\n",
        "\n",
        "\n",
        "বোঝা গেল যে ঝিঁঝিঁপোকার ডাকের সাথে তাপমাত্রা একটা সম্পর্ক আছে। তাপমাত্রা বাড়লে ঝিঁঝিঁপোকার ডাকার ফ্রিকোয়েন্সি বেড়ে যায়। এবং এই ডাকার ফ্রিকোয়েন্সি তাপমাত্রা সাথে অনেকটাই লিনিয়ার। মানে, তাপমাত্রা বাড়লে ডাকের ফ্রিকুয়েন্সি বাড়ে। ব্যাপারটাকে উল্টো করে ধরলে বলা যায়, ঝিঁঝিঁপোকার ডাককে ঠিকমত গুনতে পারলে ওই মুহূর্তের তাপমাত্রা বের করা সম্ভব হবে। ফুঁপিমা এর নোটবুক থেকে দেখা গেল, উনি একেকদিনের ঝিঁঝিঁপোকার ডাক এবং তাপমাত্রা পাশাপাশি লিখে সেটার মধ্যে একটা যোগসুত্র বের করেছিলেন সেগুলোকে প্লট করে। পুরো ১ মিনিটের ডাক রেকর্ড না করে তার প্রতি ১৫ সেকেন্ডের ঝিঁঝিঁপোকার ডাক এর সাথে তাপমাত্রাকে প্লটিংয়েই বোঝা গেল সেই লিনিয়ার সম্পর্ককে। \n",
        "\n",
        "ঝিঁঝিঁপোকার ডাক বেড়ে যাওয়া মানে তাপমাত্রা বেড়ে যাওয়া। সেখান থেকে একটা ফর্মুলা বের করেছিলেন ওই সময়। ওই ফর্মুলা দিয়ে আমাদেরকে কেউ ঝিঁঝিঁপোকার ডাক এর সংখ্যা বললে তার করেসপন্ডিং ওই সময়ে কত তাপমাত্রা হবে সেটা বের করা যাবে ওই ফর্মুলা দিয়ে। তাহলে তো আর সেটা মেশিন লার্নিং হলো না। ফর্মুলা হচ্ছে একটা রুল বেইজড সিস্টেম, যা মেশিন ডেটা থেকে শেখে না। আমি এই মুহূর্তে ফর্মুলাটা আমাদের মেশিনের কাছে আগে থেকে বলছি না, কারণ আমাদের ফুঁপিমা নোটবুক থেকে ডেটা সরাসরি মেশিনে দিয়ে দেবো - সে তার ফর্মুলা বের করতে পারে কিনা? যদি সে ইনপুট ডেটা থেকেই ফর্মুলা বের করতে পারে তাহলে আমরা বুঝে যাবো আমাদের মেশিন শিখছে। সে একটা লার্নিং মেশিন। ডেটা পাল্টে গেলে আবার সে নতুন ফর্মুলা দেবে। \n",
        "\n",
        "রাজি তো? আবারো বলছি - আমরা মেশিনকে আগে থেকে ফর্মুলা বলবো না। দেখি সে ফর্মুলা বের করতে পারে কিনা?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AC3EQFi20buB"
      },
      "source": [
        "## প্রবলেম স্টেটমেন্ট\n",
        "\n",
        "আমরা ঝিঝি পোকার 15 সেকেন্ডের ডাকের সংখ্যা বলবো, মেশিন কে আমাদেরকে বলতে হবে এখনকার তাপমাত্রা কত? এই মুহূর্তে আমাদের কাছে 55 টা রেকর্ড আছে যেখানে 15 সেকেন্ডের ঝিঝি পোকার ডাকের করেসপন্ডিং তাপমাত্রা দেয়া আছে টেবিলে। আপনারা শূন্য থেকে পাইথন মেশিন লার্নিং বইটা দেখতে পারেন। পাশাপাশি সেই ডাটা সেটের লিংক নিচে দেয়া হল।\n",
        "\n",
        "ব্যাপারটাকে আমি তিন ভাবে করতে পারি। \n",
        "\n",
        "শুধুমাত্র এটুকু বলতে পারি, প্রথম দুটো মেশিন লার্নিং নয়। "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fA93WUy1zzWf"
      },
      "source": [
        "## শুরুতেই ডিপেন্ডেন্সিগুলোকে ইমপোর্ট \n",
        "\n",
        "১. প্রথমেই টেন্সর-ফ্লো, এটাকে আমরা `tf` বলবো সুবিধার জন্য। \n",
        "\n",
        "২. টেন্সর-ফ্লো আর নামপাই খুব কাছের জিনিস। নামপাইকে শর্ট করে `np`, যা আমাদেরকে দেবে C++ এর গতি। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X9uIpOS2zx7k",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQOoosIKmx4y",
        "colab_type": "text"
      },
      "source": [
        "# টেন্সর-ফ্লো ২.০\n",
        "\n",
        "বইটা যখন বাজারে যাবে, আমি ধারণা করি তখন টেন্সর-ফ্লো ২আগে দেখে নেই আমাদের টেন্সর-ফ্লো এর কতো ভার্সন ইনস্টল করা আছে। অন্য ভার্সন থাকলে সেটাকে আপগ্রেড করে নেবো নতুন ভার্শনে। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y_WQEM5MGmg3",
        "outputId": "cc927f1a-eb14-4315-88f5-9a6322ef6cf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.14.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxNZxGXBmx43",
        "colab_type": "text"
      },
      "source": [
        "আপগ্রেড করে নিচ্ছি ২.০তে। এমুহুর্তে দরকার না থাকলেও আমরা পুরো বইটা টেন্সর-ফ্লো ২.০ দিয়ে সাঁজাতে চাই প্লাটফর্মের কনসিস্টেন্সির জন্য। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kanaYotimx43",
        "colab_type": "code",
        "outputId": "177230ae-f7e4-46f0-887e-e28fad05a125",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "!pip install -q tensorflow==2.0.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 86.3MB 31.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.3MB 24.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 501kB 50.4MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbTedodSoseW",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGce_BWknhcf",
        "colab_type": "code",
        "outputId": "459bafcb-3d40-437c-c6b4-111a1673b363",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0-rc1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-Sop5lyDVwL",
        "colab_type": "text"
      },
      "source": [
        "## টেন্সর-ফ্লো ২.x সিলেকশন\n",
        "\n",
        "পুরো বইকে কনসিস্টেন্ট রাখার জন্য আমরা নিচের এই কোড ব্যবহার করবো যাতে গুগল কোলাব/জুপিটার নোটবুকে টেন্সর-ফ্লো ২.x সিলেক্ট করতে পারে। রিসেট করে নিন গুগল কোলাবের সব রানটাইম। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alFHey8JD_E4",
        "colab_type": "code",
        "outputId": "351272e7-6a24-49fa-b307-dd8d6d519665",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try:\n",
        "  # শুধুমাত্র টেন্সর-ফ্লো ২.x ব্যবহার করবো \n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "keras = tf.keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BK7k3vIUhoqZ"
      },
      "source": [
        "## ডেটা প্লটিং\n",
        "\n",
        "শুরুতেই আমরা ডাটাগুলোকে দুটো লিস্টে ভাগ করি। প্রথম লিস্ট 'chirp15s' যেখানে আমরা ১৫ সেকেন্ডে ঝিঁঝিঁ পোকার ডাকের সংখ্যা রেকর্ড করেছি। প্রথম দুটো রেকর্ড দেখলে বোঝা যায় ঝিঁঝিঁপোকা ১৫ সেকেন্ডে ৪৪ এবং ৪৬ বার ডেকেছে। পরের লিস্টে হচ্ছে তাপমাত্রা, যা সেলসিয়াস \"temp_celsius\" রেকর্ড করা হয়েছে। সেলসিয়াস মানে আমাদের সেন্টিগ্রেড। বোঝার সুবিধার জন্য এখানে একটা ফর লুপে আমরা ঝিঁঝিঁপোকার ডাক এর পাশাপাশি তাপমাত্রা ফেলে দিচ্ছি। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gg4pn6aI1vms",
        "outputId": "6dc8006a-610c-4c3c-a44a-d2ef6e466841",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 971
        }
      },
      "source": [
        "chips_15s    = np.array([44.000,46.400,43.600,35.000,35.000,32.600,28.900,27.700,25.500,20.375,12.500,37.000,37.500,36.500,36.200,33.000,43.000,46.000,29.000,31.700,31.000,28.750,23.500,32.400,31.000,29.500,22.500,20.600,35.000,33.100,31.500,28.800,21.300,37.800,37.000,37.100,36.200,31.400,30.200,31.300,26.100,25.200,23.660,22.250,17.500,15.500,14.750,15.000,14.000,18.500,27.700,26.000,21.700,12.500,12.500],  dtype=float)\n",
        "temp_celsius = np.array([26.944, 25.833, 25.556, 23.056, 21.389, 20.000, 18.889, 18.333, 16.389, 13.889, 12.778, 24.583, 23.333, 23.333, 22.500, 18.889, 25.278, 25.833, 20.278, 20.278, 20.000, 18.889, 15.000, 21.111, 20.556, 19.444, 16.250, 14.722, 22.222, 21.667, 20.556, 19.167, 15.556, 23.889, 22.917, 22.500, 21.111, 19.722, 18.889, 20.556, 17.222, 17.222, 16.111, 16.667, 13.611, 12.778, 11.111, 11.667, 10.000, 11.111, 18.333, 17.222, 15.000, 10.417, 9.5833],  dtype=float)\n",
        "\n",
        "for i,c in enumerate(chips_15s):\n",
        "  print(\"{} Chirps in 15 Seconds = {} degrees Celsius (C)\".format(c, temp_celsius[i]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "44.0 Chirps in 15 Seconds = 26.944 degrees Celsius (C)\n",
            "46.4 Chirps in 15 Seconds = 25.833 degrees Celsius (C)\n",
            "43.6 Chirps in 15 Seconds = 25.556 degrees Celsius (C)\n",
            "35.0 Chirps in 15 Seconds = 23.056 degrees Celsius (C)\n",
            "35.0 Chirps in 15 Seconds = 21.389 degrees Celsius (C)\n",
            "32.6 Chirps in 15 Seconds = 20.0 degrees Celsius (C)\n",
            "28.9 Chirps in 15 Seconds = 18.889 degrees Celsius (C)\n",
            "27.7 Chirps in 15 Seconds = 18.333 degrees Celsius (C)\n",
            "25.5 Chirps in 15 Seconds = 16.389 degrees Celsius (C)\n",
            "20.375 Chirps in 15 Seconds = 13.889 degrees Celsius (C)\n",
            "12.5 Chirps in 15 Seconds = 12.778 degrees Celsius (C)\n",
            "37.0 Chirps in 15 Seconds = 24.583 degrees Celsius (C)\n",
            "37.5 Chirps in 15 Seconds = 23.333 degrees Celsius (C)\n",
            "36.5 Chirps in 15 Seconds = 23.333 degrees Celsius (C)\n",
            "36.2 Chirps in 15 Seconds = 22.5 degrees Celsius (C)\n",
            "33.0 Chirps in 15 Seconds = 18.889 degrees Celsius (C)\n",
            "43.0 Chirps in 15 Seconds = 25.278 degrees Celsius (C)\n",
            "46.0 Chirps in 15 Seconds = 25.833 degrees Celsius (C)\n",
            "29.0 Chirps in 15 Seconds = 20.278 degrees Celsius (C)\n",
            "31.7 Chirps in 15 Seconds = 20.278 degrees Celsius (C)\n",
            "31.0 Chirps in 15 Seconds = 20.0 degrees Celsius (C)\n",
            "28.75 Chirps in 15 Seconds = 18.889 degrees Celsius (C)\n",
            "23.5 Chirps in 15 Seconds = 15.0 degrees Celsius (C)\n",
            "32.4 Chirps in 15 Seconds = 21.111 degrees Celsius (C)\n",
            "31.0 Chirps in 15 Seconds = 20.556 degrees Celsius (C)\n",
            "29.5 Chirps in 15 Seconds = 19.444 degrees Celsius (C)\n",
            "22.5 Chirps in 15 Seconds = 16.25 degrees Celsius (C)\n",
            "20.6 Chirps in 15 Seconds = 14.722 degrees Celsius (C)\n",
            "35.0 Chirps in 15 Seconds = 22.222 degrees Celsius (C)\n",
            "33.1 Chirps in 15 Seconds = 21.667 degrees Celsius (C)\n",
            "31.5 Chirps in 15 Seconds = 20.556 degrees Celsius (C)\n",
            "28.8 Chirps in 15 Seconds = 19.167 degrees Celsius (C)\n",
            "21.3 Chirps in 15 Seconds = 15.556 degrees Celsius (C)\n",
            "37.8 Chirps in 15 Seconds = 23.889 degrees Celsius (C)\n",
            "37.0 Chirps in 15 Seconds = 22.917 degrees Celsius (C)\n",
            "37.1 Chirps in 15 Seconds = 22.5 degrees Celsius (C)\n",
            "36.2 Chirps in 15 Seconds = 21.111 degrees Celsius (C)\n",
            "31.4 Chirps in 15 Seconds = 19.722 degrees Celsius (C)\n",
            "30.2 Chirps in 15 Seconds = 18.889 degrees Celsius (C)\n",
            "31.3 Chirps in 15 Seconds = 20.556 degrees Celsius (C)\n",
            "26.1 Chirps in 15 Seconds = 17.222 degrees Celsius (C)\n",
            "25.2 Chirps in 15 Seconds = 17.222 degrees Celsius (C)\n",
            "23.66 Chirps in 15 Seconds = 16.111 degrees Celsius (C)\n",
            "22.25 Chirps in 15 Seconds = 16.667 degrees Celsius (C)\n",
            "17.5 Chirps in 15 Seconds = 13.611 degrees Celsius (C)\n",
            "15.5 Chirps in 15 Seconds = 12.778 degrees Celsius (C)\n",
            "14.75 Chirps in 15 Seconds = 11.111 degrees Celsius (C)\n",
            "15.0 Chirps in 15 Seconds = 11.667 degrees Celsius (C)\n",
            "14.0 Chirps in 15 Seconds = 10.0 degrees Celsius (C)\n",
            "18.5 Chirps in 15 Seconds = 11.111 degrees Celsius (C)\n",
            "27.7 Chirps in 15 Seconds = 18.333 degrees Celsius (C)\n",
            "26.0 Chirps in 15 Seconds = 17.222 degrees Celsius (C)\n",
            "21.7 Chirps in 15 Seconds = 15.0 degrees Celsius (C)\n",
            "12.5 Chirps in 15 Seconds = 10.417 degrees Celsius (C)\n",
            "12.5 Chirps in 15 Seconds = 9.5833 degrees Celsius (C)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vZ33rAJJdnh5"
      },
      "source": [
        "## ডেটা ভিজ্যুয়ালাইজেশন\n",
        "\n",
        "১. আমরা পুরো ডাটাসেটকে এক্স এবং ওয়াই এক্সিসে প্লট করতে পারি। যেহেতু আমরা আগেই দেখেছি ঝিঁঝিঁপোকার ডাক এবং তাপমাত্রার সম্পর্কটা লিনিয়ার, সেখানে একটা 'বেস্ট ফিট লাইন' আমাদেরকে ভবিষ্যৎ যে কোনো ডাকের সংখ্যার করেসপন্ডিং তাপমাত্রা দেখাতে পারবে। এক্স অ্যাক্সিস যদি ঝিঁঝিঁপোকার ডাকের সংখ্যা হয় তাহলে ওয়াই এক্সিসে তারপর করেসপন্ডিং তাপমাত্রা পাওয়া যাবে। \n",
        "\n",
        "২. নতুন ঝিঁঝিঁপোকার ডাক এর সংখ্যা যেটা এখানে নেই, সেটাও এই প্লটিং এ এক্স এক্সিসের যে অংশটা ওয়াই এক্সিসের এর সঙ্গে স্পর্শ করেছে সেটাই প্রেডিক্টেড তাপমাত্রা হবে।\n",
        "\n",
        "তবে, এই ছবিতে আমরা সেটা না দেখিয়ে সামনে দেখানোর প্ল্যান করছি। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K4dyHUdhdLgE",
        "outputId": "ec707d24-3094-4209-9afa-0c2f58199060",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X = chips_15s\n",
        "y = temp_celsius\n",
        "\n",
        "plt.scatter(X, y, color='red')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAGVtJREFUeJzt3X/sHPV95/HnC2Mu/LAE2N9a4F/f\nKIc4IdSY9ntuTqRVQAl1fFFIoioFfc9yGiT3QnKCE7qUnP8g156l3PUgrZSG6JtgcHMWSXSAghI3\nwaJIiCoFvqaGGExrjrMbO479xU4KkasS4/f9MbPyej2zO7s7uzu783pIX+3OZ2b2+/mO4L3j97zn\nPYoIzMysPs4b9QTMzGy4HPjNzGrGgd/MrGYc+M3MasaB38ysZhz4zcxqxoHfzKxmHPjNzGrGgd/M\nrGbO77SBpFXAXwLLgQDmIuLPJX0buDrd7FLgFxGxNmP/A8BbwDvAqYiY6fQ7ly1bFtPT00X/BjOz\n2tu9e/cbETFVZNuOgR84BdwVES9IWgLslrQrIn6/sYGke4F/avMZN0TEG0UmBDA9Pc38/HzRzc3M\nak/SwaLbdgz8EXEEOJK+f0vSPmAF8Er6ywR8Erixp9mamdlQdZXjlzQNXAc82zT828DRiNifs1sA\nT0jaLWlzL5M0M7PyFEn1ACDpEuAR4M6IeLNp1a3Aw212fX9EHJb0a8AuSa9GxNMZn78Z2AywevXq\notMyM7MuFTrjl7SYJOjviIhHm8bPBz4BfDtv34g4nL4eAx4D1uVsNxcRMxExMzVV6PqEmZn1oGPg\nT3P4DwD7IuK+ltUfBF6NiEM5+16cXhBG0sXATcDe/qZsZmb9KHLGfz2wEbhR0p70Z0O67hZa0jyS\nrpS0M11cDjwj6UXgOeD7EfGDkuZuZjZ8O3bA9DScd17yumPHqGfUtSJVPc8Ayln3qYyxnwIb0vev\nA+/tb4pmZhWxYwds3gwnTybLBw8mywCzs6ObV5d8566ZWVFbtpwJ+g0nTybjY8SB38ysqH/8x+7G\nK8qB38ysqLxS8zErQXfgNzMrautWuOiis8cuuigZHyMO/GZmRc3OwtwcrFkDUvI6NzdWF3ahizt3\nzcyMJMiPWaBv5TN+M7OaceA3M6sZB34zs5px4DczqxkHfjOzmnHgNzMbttZGb7ffPtTGby7nNDMb\npqxGb/fff2b9EBq/+YzfzGyYshq9tRpw4zcHfjOzdsruv1+0odsAG7858JuZ5WmkZQ4ehIgzaZh+\ngn/Rhm4DbPzmwG9mlmcQ/fezGr21GnDjNwd+M7OG1rTOwYPZ2/WThslq9PaZzwy18ZureszMILva\nRkpSPK36TcOMuNGbz/jNzCA7rRORBP9mY9h/v1XHwC9plaSnJL0i6WVJd6TjX5R0WNKe9GdDzv7r\nJf29pNck3V32H2BmVoq89E3E2Pffb1Uk1XMKuCsiXpC0BNgtaVe67ssR8b/ydpS0CPgL4EPAIeB5\nSY9HxCv9TtzMrFSrV2fn9NesgQMHhj6dQep4xh8RRyLihfT9W8A+YEXBz18HvBYRr0fE28C3gJt7\nnayZ2cBMyGMVi+gqxy9pGrgOeDYd+pyklyRtk3RZxi4rgJ80LR+i+JeGmdnwTMhjFYsoHPglXQI8\nAtwZEW8C9wPvAdYCR4B7+5mIpM2S5iXNLyws9PNRZma9mZ1N0jqnTyevExj0oWDgl7SYJOjviIhH\nASLiaES8ExGnga+TpHVaHQZWNS2vTMfOERFzETETETNTU1Pd/A1mNmxltzGwoSpS1SPgAWBfRNzX\nNH5F02YfB/Zm7P48cJWkd0u6ALgFeLy/KZvZSA2ijYENVZEz/uuBjcCNLaWb/1PSjyW9BNwA/GcA\nSVdK2gkQEaeAzwE/JLko/J2IeHkQf4iZDckg2hjYUCmy7kobsZmZmZifnx/1NMwsy3nnZd/NKiW5\ncRsJSbsjYqbItr5z18y6k9euYIDdJK1cDvxm1p0a1btPKgd+M+tOjerdJ5W7c5pZ90bcXdL64zN+\nMyuXa/wrz2f8ZlaerJ72mzcn7/0vhMrwGb+Zlcc1/mPBgd/MypPX076fRxVa6Rz4zaw8rvEfCw78\nZlYe1/iPBQd+M2uvSJVOY5uNG+HCC2HpUtf4V5ireswsX5EqndZtjh9PzvK/+U0H/IpykzYzyzc9\n3fk5tEW2sYFzkzYzK0eRKh1X8owdB36zOun2rtoiVTqu5Bk7DvxmddHLk7PyqnQ2bDjzBfLLX8Li\nxedu40qeynLgN6uLXu6qzerEuWkTbN9+5gvk+PFknSt5xoareszqotdcfGsnzunpc79A3n4bLrkE\n3nijrynacPiM36wuysrF+2Lu2OsY+CWtkvSUpFckvSzpjnT8TyW9KuklSY9JujRn/wPpQ9n3SHKN\nptmolHVXrS/mjr0iZ/yngLsi4hrgfcBnJV0D7AKujYhfB/4B+EKbz7ghItYWrTE1swHo9slZeRVA\nWV8gUnLB18ZCxxx/RBwBjqTv35K0D1gREU80bfa3wO8NZopmVpqiT87qdMfu3/wNfO1rycVdSF63\nb4frr/dF3THQVY5f0jRwHfBsy6pPA3+Vs1sAT0jaLWlztxM0sxHoVAG0c+eZoJ+13iqtcOCXdAnw\nCHBnRLzZNL6FJB2UVwz8/oj4DeDDJGmi38n5/M2S5iXNLywsFP4DzKwLRW/g6nQB1xd4x1qhwC9p\nMUnQ3xERjzaNfwr4CDAbOU1/IuJw+noMeAxYl7PdXETMRMTM1NRUV3+EmRXQzQ1cnS7g+gLvWCtS\n1SPgAWBfRNzXNL4e+Dzw0Yg4mbPvxZKWNN4DNwF7y5i4mXWpmxu4OlUAue/+WCtyxn89sBG4MS3J\n3CNpA/AVYAmwKx37GoCkKyXtTPddDjwj6UXgOeD7EfGD8v8MM+uom/RMpwqgbiuErFLcltmsLtw+\neaK5LbOZncvpGUs58JuNi25bKrdyesZSbtJmNg6KPAKxiKI3cNlE8xm/2TjopaWyWQ4HfrNx4Bum\nrEQO/GbjoKwbpvq9TmATwYHfbByUUZHTy6MXbSI58JuNgzIqcnydwFIO/GZV0i4VMzub3Gh1+nTy\n2m11TrfXCZwWmlgu5zSrirJKNvOsXp19527WdYJBz8VGymf8ZlUx6FRMN9cJnBaaaA78ZlUx6JLN\nbq4TuHx0ojnVY1YV3aRielX0zt1hzMVGxmf8ZlVRpSZqVZqLlc6B32yQmitjli2DJUuSNIuULLdW\n7VSliVqV5mKlcz9+s0FprYzJsngxPPigA6r1zf34zaogqzKm1a9+5UoZGzoHfrMsZdy8VLQC5uBB\n3xxlQ+XAb9aqrJ423VTAuGeODZEDv1mrsm5eyqqMyeObo2yIOgZ+SaskPSXpFUkvS7ojHb9c0i5J\n+9PXy3L235Rus1/SprL/ALPSlXXz0uwsbNoEixYlyxK8613Ffq/75NgAFTnjPwXcFRHXAO8DPivp\nGuBu4MmIuAp4Ml0+i6TLgXuA3wLWAffkfUGYVUaZve+3b4d33kmWI5JAvnRp+893+2QbsI6BPyKO\nRMQL6fu3gH3ACuBmYHu62XbgYxm7/y6wKyJORMTPgV3A+jImbjYwZd28lJcyanxe3ue7T44NWFc5\nfknTwHXAs8DyiDiSrvoZsDxjlxXAT5qWD6VjWZ+9WdK8pPmFhYVupmVWrrJuXspLDZ040f7z3SfH\nBqxw4Jd0CfAIcGdEvNm8LpK7wPq6Eywi5iJiJiJmpqam+vkos/712/se8lNDEcnZ+9at2Z9fVqrJ\nLEehwC9pMUnQ3xERj6bDRyVdka6/AjiWsethYFXT8sp0zGzytavqaZe3d58cG7AiVT0CHgD2RcR9\nTaseBxpVOpuA72bs/kPgJkmXpRd1b0rHzCZfc8ooS17e3n1ybMCKnPFfD2wEbpS0J/3ZAHwJ+JCk\n/cAH02UkzUj6BkBEnAD+BHg+/fnjdMxsvPRaXtlIGUnZ6/Py9mWkmsxyuEmbWSdZzdYuuqi7s/Dp\n6ez+9mvWJIHdrE9u0mZWpjLKK523twpx4DdrlpXSKaO80nl7qxCneswa8lI6F14Ix4+fu73TNFYh\nTvWY9aLXO23NxowDv1lDXurm+PH2aZphNFRz0zYr0fmjnoBZZVx+eXZKpyErrdOaHmrcmAXl5e+H\n8TusVpzjN2tYtiw/8Ofl84dRpulSUCvAOX6zXpxoc29ht5U9ZTZUc9M2K5kDv1lDuyZo3TZOK7Oh\nmpu2Wckc+M0atm6FCy44d3zx4vwKnmHcmOWbv6xkDvxWD7ffDuefn1TlnH9+stxqdha2bTv7CVlL\nl8KDD+ZfRB3GjVm++ctK5ou7Nvluvx3uv//c8c98Br761eHPx2wAfHHXrNncXHfjZhPOgd8mX+Nh\n50XHzSacA79NvkWLuhs3m3AO/Db5Gne5Fh03m3Bu2WCTr3EBd24uSe8sWpQEfV/YtZpy4Ld6+OpX\nHejNUk71mJnVTMczfknbgI8AxyLi2nTs28DV6SaXAr+IiLUZ+x4A3gLeAU4VrTE1M7PBKZLqeQj4\nCvCXjYGI+P3Ge0n3Av/UZv8bIuKNXidoZmbl6hj4I+JpSdNZ6yQJ+CRwY7nTMjOzQek3x//bwNGI\n2J+zPoAnJO2W5No5M7MK6Dfw3wo83Gb9+yPiN4APA5+V9Dt5G0raLGle0vzCwkKf07Kx1e4Rgzt2\nJA9LkZKfZcv8CEKzHvRczinpfOATwG/mbRMRh9PXY5IeA9YBT+dsOwfMQdKkrdd52Rhr94hBgE9/\nGt5++8zy8ePwB3+QvHenSrPC+jnj/yDwakQcylop6WJJSxrvgZuAvX38Ppt0W7acCfoNJ08m41u2\nnB30G371q2SdmRXWMfBLehj4EXC1pEOSbktX3UJLmkfSlZJ2povLgWckvQg8B3w/In5Q3tRt4rR7\nxGC7xwz6EYRmXSlS1XNrzvinMsZ+CmxI378OvLfP+VmdrF6d/VDxxiMGs9Y1rzezQnznrlVHu0cM\n9vJYRDPL5MBvnbWrtClTu0cM9vJYRDPL5EcvWnutlTaQnIX7ma9mleJHL1p52lXamNlYcuC39tpV\n2gzasFJMZjXjwG/t5VXMDLqSppFiOngQIs7czOXgb9Y3B35rL6vSRkoC8SDPwp1iMhsYB35rr7nS\nBpKg3ygIGORZ+ChTTGYTzoHfOpudhQMHkuDfWgWWdxbeb35+VCkmsxpw4Lfiip6Fl5Gfb3czl5n1\nxYHfiit6Fl5Gfr7dzVxm1hcHfiuu6Fl4Wfn5Rorp9Onk1UHfrBQO/FZc0bNw5+fNKs2B37pT5Czc\n+XmzSnPgt/LvkHV+3qzSen70ok2Ido877CdQNzpqmlnl+Iy/7nyHrFntOPDXne+QNasdB/66cwWO\nWe048NedK3DMaqdj4Je0TdIxSXubxr4o6bCkPenPhpx910v6e0mvSbq7zIlbSVorcJYuhQsvhI0b\ne6/wcR99s0orcsb/ELA+Y/zLEbE2/dnZulLSIuAvgA8D1wC3Srqmn8nagDRq87/5Tfjnf4bjx3vv\nseM++maV1zHwR8TTwIkePnsd8FpEvB4RbwPfAm7u4XNsWMqo8HGVkFnl9ZPj/5ykl9JU0GUZ61cA\nP2laPpSOZZK0WdK8pPmFhYU+pmW5OqVgyqjwcZWQWeX1GvjvB94DrAWOAPf2O5GImIuImYiYmZqa\n6vfjrFWRFEwZFT6uEjKrvJ4Cf0QcjYh3IuI08HWStE6rw8CqpuWV6ZiNQpEUTBkVPq4SMqu8ngK/\npCuaFj8O7M3Y7HngKknvlnQBcAvweC+/z0pQJAVTRo8d9+kxqzxF66P0WjeQHgY+ACwDjgL3pMtr\ngQAOAH8YEUckXQl8IyI2pPtuAP4MWARsi4hCp30zMzMxPz/fw59juaank/ROqzVrkooeMxtrknZH\nxEyRbTs2aYuIWzOGH8jZ9qfAhqblncA5pZ42Alu3nt2MDZyCMasp37lbF07BmFnKbZnrxK2SzQyf\n8ZuZ1Y4Dv5lZzTjw14GbpplZE+f4J92gHq1oZmPLZ/yTzk3TzKyFA/8oDSMF46ZpZtbCgX9UhtW3\n3k3TzKyFA/+oDCsF46ZpZtbCgX9UhpWC8R27ZtbCVT2jsnp1dtO0QaRgfMeumTXxGf+oOAVjZiPi\nwD8qTsGY2Yg41TNKTsGY2Qj4jN/MrGYmJ/APsx+Ne9+Y2RibjFTPMPvRuPeNmY25js/cHYWun7k7\nzOfJ+tm1ZlZB3Txzt2OqR9I2Scck7W0a+1NJr0p6SdJjki7N2feApB9L2iNpcE9PH2Y/Gve+MbMx\nVyTH/xCwvmVsF3BtRPw68A/AF9rsf0NErC36TdSTYfajce8bMxtzHQN/RDwNnGgZeyIiTqWLfwus\nHMDcihvmzVC+8crMxlwZVT2fBv4qZ10AT0jaLWlzCb8r2zBvhvKNV2Y25gpd3JU0DXwvIq5tGd8C\nzACfiIwPkrQiIg5L+jWS9NB/Sv8FkfU7NgObAVavXv2bB7MuoJqZWaZSL+62+SWfAj4CzGYFfYCI\nOJy+HgMeA9blfV5EzEXETETMTE1N9TotMzProKfAL2k98HngoxFxMmebiyUtabwHbgL2Zm1rZmbD\nU6Sc82HgR8DVkg5Jug34CrAE2JWWan4t3fZKSTvTXZcDz0h6EXgO+H5E/GAgf4WZmRXW8c7diLg1\nY/iBnG1/CmxI378OvLev2ZmZWekmp1ePmZkV4sDfiRuymdmEmYwmbYPihmxmNoF8xt/Oli1ngn7D\nyZPJuJnZmHLgb8cN2cxsAjnwt+OGbGY2gRz423FDNjObQA78eXbsOJPjX7QoGRtFQzZXFZlZyVzV\nk6W1muedd86c6Q876LuqyMxKNhmPXixbVR6vWJV5mFnlDaU750SrSjVPVeZhZhPFgT9LVap5qjIP\nM5soDvxZqlLNU5V5mNlEceDPUpXHK1ZlHmY2UXxx18xsAtTz4q7r3c3MCpmMOn7Xu5uZFTYZZ/zu\nomlmVthkBH7Xu5uZFVYo8EvaJumYpL1NY5dL2iVpf/p6Wc6+m9Jt9kvaVNbEz+J6dzOzwoqe8T8E\nrG8Zuxt4MiKuAp5Ml88i6XLgHuC3gHXAPXlfEH3ptd7dF4TNrIYKBf6IeBo40TJ8M7A9fb8d+FjG\nrr8L7IqIExHxc2AX536B9K+XevfGBeGDByHizAVhB38zm3D95PiXR8SR9P3PgOUZ26wAftK0fCgd\nK9/sbNK47PTp5LVTNY8vCJtZTZVycTeSu8D6uhNM0mZJ85LmFxYWyphWe74gbGY11U/gPyrpCoD0\n9VjGNoeBVU3LK9Oxc0TEXETMRMTM1NRUH9MqyBeEzaym+gn8jwONKp1NwHcztvkhcJOky9KLujel\nY+Xr9kKtG6CZWU0VLed8GPgRcLWkQ5JuA74EfEjSfuCD6TKSZiR9AyAiTgB/Ajyf/vxxOlauXi7U\nugGamdXUZDRp85OqzKzm6tekzRdqzcwKm4zA7wu1ZmaFTUbg94VaM7PCJiPw+0KtmVlhk9GPH5Ig\n70BvZtbRZJzxm5lZYQ78ZmY148BvZlYzDvxmZjXjwG9mVjOVbNkgaQHI6MEwUsuAN0Y9iS55zsPh\nOQ+H59zemogo1Nq4koG/iiTNF+2DURWe83B4zsPhOZfHqR4zs5px4DczqxkH/uLmRj2BHnjOw+E5\nD4fnXBLn+M3MasZn/GZmNePAn0HSNknHJO1tGvuipMOS9qQ/G0Y5x1aSVkl6StIrkl6WdEc6frmk\nXZL2p6+XjXqu0Ha+lT3Okt4l6TlJL6Zz/m/p+LslPSvpNUnflnTBqOfa0GbOD0n6f03Hee2o59pK\n0iJJfyfpe+lyZY9zQ8acK3mcHfizPQSszxj/ckSsTX92DnlOnZwC7oqIa4D3AZ+VdA1wN/BkRFwF\nPJkuV0HefKG6x/lfgBsj4r3AWmC9pPcB/4Nkzv8a+Dlw2wjn2CpvzgD/pek47xndFHPdAexrWq7y\ncW5onTNU8Dg78GeIiKeB8h8KP0ARcSQiXkjfv0XyH98K4GZge7rZduBjo5nh2drMt7Ii8ct0cXH6\nE8CNwP9JxytzjKHtnCtN0krg3wPfSJdFhY8znDvnKnPg787nJL2UpoIqkTLJImkauA54FlgeEUfS\nVT8Dlo9oWrla5gsVPs7pP+X3AMeAXcD/BX4REafSTQ5RsS+w1jlHROM4b02P85cl/asRTjHLnwGf\nB06ny0up+HHm3Dk3VO44O/AXdz/wHpJ/Lh8B7h3tdLJJugR4BLgzIt5sXhdJCVelzvYy5lvp4xwR\n70TEWmAlsA74NyOeUketc5Z0LfAFkrn/W+By4I9GOMWzSPoIcCwido96LkW1mXMlj7MDf0ERcTT9\nH+g08HWS/+krRdJikiC6IyIeTYePSroiXX8FyVlfJWTNdxyOM0BE/AJ4Cvh3wKWSGk+zWwkcHtnE\n2mia8/o01RYR8S/Ag1TrOF8PfFTSAeBbJCmeP6fax/mcOUv631U9zg78BTWCZ+rjwN68bUchzYE+\nAOyLiPuaVj0ObErfbwK+O+y5Zcmbb5WPs6QpSZem7y8EPkRybeIp4PfSzSpzjCF3zq82nQyIJFde\nmeMcEV+IiJURMQ3cAvx1RMxS4eOcM+f/UNXjPDnP3C2RpIeBDwDLJB0C7gE+kJZiBXAA+MORTTDb\n9cBG4MdpPhfgvwJfAr4j6TaSjqefHNH8WuXN99YKH+crgO2SFpGcNH0nIr4n6RXgW5L+O/B3JF9o\nVZE357+WNAUI2AP8x1FOsqA/orrHOc+OKh5n37lrZlYzTvWYmdWMA7+ZWc048JuZ1YwDv5lZzTjw\nm5nVjAO/mVnNOPCbmdWMA7+ZWc38f9LrYPUSwRdOAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "idC7mALvkU1Z"
      },
      "source": [
        "৩. আমাদের যেহেতু ৫৫টা রেকর্ড আছে, আর রেকর্ডগুলো প্লট করলে প্রায় একটা সরল রেখা হয়, তাহলে কি একটা সরলরেখার ইকুয়েশন বের করতে পারি আমরা? সরলরেখার ইকুয়েশনে আমরা ওয়াই ভ্যালু, মানে আমাদের 'প্রেডিক্টেড' তাপমাত্রা বের করতে চাইলে, এক্স ভ্যালুতে ঝিঁঝিঁপোকার ডাক এর সংখ্যা বসালেই তো উত্তর পাবার কথা, ঠিক না? সরলরেখার ইকুয়েশন যদি y = mx + b হয় তাহলে তো 'm' যাকে আমরা স্লোপ বা ঢাল বলছি, সেটা বের করা যাবে না? এর পাশাপাশি ওয়াই ইন্টারসেপ্ট 'b' পাওয়াটা তো কঠিন কিছু না। এই পুরো ব্যাপারটা আমরা দু ভাবে করতে পারি। প্রথমটা সাধারণ অংক, ফর্মুলা থেকে, পরেরটা আমরা দেখবো পাইথন দিয়ে। কোনটাই মেশিন লার্নিং নয়।\n",
        "\n",
        "** আমরা সরল রেখার অংক মানে ইকুয়েশন (y = mx + b) নিয়ে আলাপ করছি। আগের বই \"শূন্য থেকে পাইথন মেশিন লার্নিং\" বইয়ে বড় করে আলাপ করেছিলাম আগে। **\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2yozKL-hrFUL"
      },
      "source": [
        "## এই ফর্মুলাটা নিরেট অংকে করি \n",
        "\n",
        "*কোন মেশিন লার্নিং নয় *\n",
        "\n",
        "আমাদের ৫৫টা ডেটাপয়েন্ট থেকে অংকে ফর্মুলাতে নিয়ে আসি। সরল রেখার অংক। ফর্মুলা নিচে দেখুন। \n",
        "\n",
        "আমাদের \"chips_15s\" মানে X এর সব ভ্যালুগুলোকে যোগ করি। \n",
        "\n",
        "X_sum =\n",
        "\n",
        "44.000+46.400+43.600+35.000+35.000+32.600+28.900+27.700+25.500+20.375+12.500+37.000+37.500+36.500+36.200+33.000+43.000+46.000+29.000+31.700+31.000+28.750+23.500+32.400+31.000+29.500+22.500+20.600+35.000+33.100+31.500+28.800+21.300+37.800+37.000+37.100+36.200+31.400+30.200+31.300+26.100+25.200+23.660+22.250+17.500+15.500+14.750+15.000+14.000+18.500+27.700+26.000+21.700+12.500+12.500= 1584.285\n",
        "\n",
        "এখন আমরা 'temp_celsius' মানে y ভ্যালুগুলোকে যোগ করি। \n",
        "\n",
        "y_sum =\n",
        "\n",
        "26.944+ 25.833+ 25.556+ 23.056+ 21.389+ 20.000+ 18.889+ 18.333+ 16.389+ 13.889+ 12.778+ 24.583+ 23.333+ 23.333+ 22.500+ 18.889+ 25.278+ 25.833+ 20.278+ 20.278+ 20.000+ 18.889+ 15.000+ 21.111+ 20.556+ 19.444+ 16.250+ 14.722+ 22.222+ 21.667+ 20.556+ 19.167+ 15.556+ 23.889+ 22.917+ 22.500+ 21.111+ 19.722+ 18.889+ 20.556+ 17.222+ 17.222+ 16.111+ 16.667+ 13.611+ 12.778+ 11.111+ 11.667+ 10.000+ 11.111+ 18.333+ 17.222+ 15.000+ 10.417+ 9.5833= 1030.1403\n",
        "\n",
        "এখন প্রতিটা ডেটা পয়েন্টের প্রোডাক্টকে যোগ করে ফেলি। \n",
        "\n",
        "Xy_sum =\n",
        "\n",
        "44.000*26.944+ 46.400*25.833+ 43.600*25.556+ 35.000*23.056+ 35.000*21.389+ 32.600*20.000+ 28.900*18.889+ 27.700*18.333+ 25.500*16.389+ 20.375*13.889+ 12.500*12.778+ 37.000*24.583+ 37.500*23.333+ 36.500*23.333+ 36.200*22.500+ 33.000*18.889+ 43.000*25.278+ 46.000*25.833+ 29.000*20.278+ 31.700*20.278+ 31.000*20.000+ 28.750*18.889+ 23.500*15.000+ 32.400*21.111+ 31.000*20.556+ 29.500*19.444+ 22.500*16.250+ 20.600*14.722+ 35.000*22.222+ 33.100*21.667+ 31.500*20.556+ 28.800*19.167+ 21.300*15.556+ 37.800*23.889+ 37.000*22.917+ 37.100*22.500+ 36.200*21.111+ 31.400*19.722+ 30.200*18.889+ 31.300*20.556+ 26.100*17.222+ 25.200*17.222+ 23.660*16.111+ 22.250*16.667+ 17.500*13.611+ 15.500*12.778+ 14.750*11.111+ 15.000*11.667+ 14.000*10.000+ 18.500*11.111+ 27.700*18.333+ 26.000*17.222+ 21.700*15.000+ 12.500*10.417+ 12.500*9.5833= 31775.986435\n",
        "\n",
        "এখন X এবং y ভ্যালুগুলোর যোগফলকে আলাদা আলাদা করে বর্গ করি।  \n",
        "\n",
        "X_square_sum =\n",
        "\n",
        "44.000^2+46.400^2+43.600^2+35.000^2+35.000^2+32.600^2+28.900^2+27.700^2+25.500^2+20.375^2+12.500^2+37.000^2+37.500^2+36.500^2+36.200^2+33.000^2+43.000^2+46.000^2+29.000^2+31.700^2+31.000^2+28.750^2+23.500^2+32.400^2+31.000^2+29.500^2+22.500^2+20.600^2+35.000^2+33.100^2+31.500^2+28.800^2+21.300^2+37.800^2+37.000^2+37.100^2+36.200^2+31.400^2+30.200^2+31.300^2+26.100^2+25.200^2+23.660^2+22.250^2+17.500^2+15.500^2+14.750^2+15.000^2+14.000^2+18.500^2+27.700^2+26.000^2+21.700^2+12.500^2+12.500^2= 49879.553725\n",
        "\n",
        "আগেই বলেছি, আমাদের ডেটাপয়েন্ট আছে ৫৫টা। মানে N=55. তাহলে বেস্ট ফিট লাইনের গ্রাডিয়েন্ট পাওয়া যাবে নিচের ফর্মুলা থেকে। আগের বইটা দেখতে পারেন। \n",
        "\n",
        "m = (N * Xy_sum) - (X_sum * y_sum) / (N * X_square_sum) - (X_sum * X_sum)\n",
        "\t = (55 * 31775.986435) - (1584.285 * 1030.1403) / (55 * 49879.553725) - (1584.285^2)\n",
        "\t = 0.49543811976\n",
        "\n",
        "এখন এই বেস্ট ফিট লাইনের ইন্টারসেপ্ট দরকার আমার লাইনের জন্য। সেটার ফর্মুলা;\n",
        "\n",
        "b = (X_square_sum * y_sum ) - (X_sum * Xy_sum) / (N * X_square_sum) - (X_sum * x_sum)\n",
        "   = (49879.553725 * 1030.1403) - (1584.285 * 31775.986435) / (55 * 49879.553725) -          (1584.285* 1584.285)\n",
        "   = 4.45863851637\n",
        "\n",
        "তাহলে আমাদের এই রিগ্রেশন লাইনের সরলরেখার ইক্যুয়েশন কি?\n",
        "\n",
        "y = 0.49543811976X + 4.45863851637"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Sp8VdefCrFUM"
      },
      "source": [
        "অংকে তো মাথা খারাপ হয়ে গেলো। একটু পাইথনে দেখি। এখনো মেশিন লার্নিং নয় কিন্তু। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9xaBzGNQjAew",
        "outputId": "9be7b965-8a83-473e-ee99-10509ac829ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from statistics import mean\n",
        "\n",
        "def best_fit_slope_and_intercept(X,y):\n",
        "    m = (((mean(X)*mean(y)) - mean(X*y)) /\n",
        "         ((mean(X)*mean(X)) - mean(X*X)))\n",
        "    \n",
        "    b = mean(y) - m*mean(X)\n",
        "    \n",
        "    return m, b\n",
        "\n",
        "m, b = best_fit_slope_and_intercept(X,y)\n",
        "\n",
        "print(m,b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.49543811977958857 4.458638516454446\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iKI0XlUmx5J",
        "colab_type": "text"
      },
      "source": [
        "y = mx + b এর হিসেবে \n",
        "\n",
        "y = 0.49543811977958857X + 4.458638516454446\n",
        "\n",
        "একদম কাছাকাছি। "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ogQmit3drFUQ"
      },
      "source": [
        "## বেস্ট ফিট লাইন \n",
        "\n",
        "আমরা একটা রিগ্রেশন লাইন টেনে ফেলি। ধরুন ১৫ সেকেন্ডের ঝিঁঝিঁপোকার ৪১ ডাকের প্রেডিকশন কি হবে? Y এক্সিসের বরাবর রেখাটা এক্স এক্সিসের কোথায় স্পর্শ করেছে সেটা দেখলেই কিন্তু পাওয়া যাবে। \n",
        "\n",
        "আচ্ছা, লাইনটা কিভাবে টানলাম সেটা নিয়ে আলাপ নয় এখন। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rWJl7uh2kWn4",
        "outputId": "adad1097-1339-4f57-dcb1-a7fd7eac95d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "# regression_line = [(m*x)+b for x in X]\n",
        "\n",
        "regression_line = []\n",
        "for x in X:\n",
        "    regression_line.append((m*x)+b)\n",
        "\n",
        "plt.scatter(X,y,color='red')\n",
        "plt.plot(X, regression_line)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wwJGmDrQ0EoB"
      },
      "source": [
        "### মেশিন লার্নিং কিছু টার্মিনোলোজি \n",
        "\n",
        " - **ফিচার** — আমাদের মডেলের ইনপুট ডেটা। আমাদের এখানে একটা ভ্যালু - ১৫ সেকেন্ডে ঝিঁঝিঁপোকার ডাকের সংখ্যা। \n",
        " \n",
        " - **লেবেল** — যেই আউটপুটটা আমাদের মডেল শিখে প্রেডিক্ট করবে। আমাদের এখানে তাপমাত্রা। \n",
        " \n",
        " - **এক্সাম্পল** — ইনপুট/আউটপুট ডেটার একটা জোড়া, যা দরকার পড়ছে ট্রেনিং এর সময়। আমাদের এখানে chips_15s এবং temp_celsius অ্যারে থেকে দুটো ডেটা একটা ইনডেক্সে দেখলে (44.000,26.944) পাওয়া যাবে।  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ziWM65lRrFUX"
      },
      "source": [
        "## মেশিন লার্নিং মডেল \n",
        "\n",
        "আমরা সাইকিট-লার্ন দিয়েও এই জিনিসটা করতে পারতাম। তবে, যেহেতু আমরা ডিপ লার্নিং মানে টেন্সর-ফ্লো নিয়ে কাজ করতে চাই, সেকারণে আমাদের শুরুতে ফোকাস থাকবে একটা সাধারণ মডেলে। বিশেষ করে বোঝার ক্ষেত্রে। একটা লিনিয়ার মডেলে নিউরাল নেটওয়ার্ক দরকার নেই, বরং নন-লিনিয়ার মডেলের জন্য দরকার নিউরাল নেটওয়ার্ক। সেগুলো নিয়ে সামনে আলাপ হবে। "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_4yDv1Y7fmmM"
      },
      "source": [
        "৪. আমাদের কাছে যেই ডাটা আছে, সেটাকে মেশিন লার্নিং মডেলে ঢুকিয়ে দেবো, দেখি মেশিন লার্নিং মডেল এর থেকে কোনো ফর্মুলা বের করতে পারে কিনা? পাশাপাশি আমরা একটা অজানা ভ্যালু দিয়ে দেখতে চাইবো সেখান থেকে প্রেডিক্টেড ভ্যালু জানাতে পারে কিনা? তাহলেই তো আমাদের কাজ হয়ে যায়, কি বলেন? আমরা যদি আমাদের ডিপ লার্নিং লাইব্রেরি টেন্সর-ফ্লোকে ঝিঁঝিঁ পোকার ডাক এর সংখ্যা এবং তার করেসপন্ডিং তাপমাত্রা দিয়ে দেই, তাহলে যদি সে ফর্মুলা দিতে পারে, তখন আমরা বলতে পারব আমাদের মেশিন লার্নিং মডেল ডেটা থেকে শিখেছে। যদি ডাটা থেকে নিজে নিজেই ফর্মুলা বের করতে পারে সেটাই কিন্তু শেখা, মানে মেশিন লার্নিং।\n",
        "\n",
        "** আমাদের এই মডেল তৈরি করতে ডিপ লার্নিং অথবা টেন্সর-ফ্লো লাগবে না। তবে ডিপ লার্নিং ফ্রেমওয়ার্ক নিয়ে ধারণা পাবার জন্য একটা অগভীর মানে 'শ্যালো' এক লেয়ারের, এক নিউরনের নেটওয়ার্ক বানানোর চেষ্টা করবো। **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzzdwSLzmx5Q",
        "colab_type": "text"
      },
      "source": [
        "## নিউরাল নেটওয়ার্কের ৫ ষ্টেপ লাইফ সাইকেল - কেরাস দিয়ে \n",
        "\n",
        "টেন্সর-ফ্লো ডিপ লার্নিং ফ্রেমওয়ার্ক হিসেবে একটু কমপ্লেক্স। সাইকিট-লার্ন এপিআইএর মতো এতো কম হাইপার-প্যারামিটার নিয়ে তাকে চালানো দুস্কর। যেহেতু ফ্রেমওয়ার্কটা অনেক শক্তিশালী, সেকারণে আমরা এটাকে এক্সেস করবো একটা হাই-লেভেল এপিআই দিয়ে যার কাজ হচ্ছে এই ভেতরের কমপ্লেক্সিটিকে লুকিয়ে রাখবে আমাদের কাছ থেকে। একারণে টেন্সর-ফ্লো ২.০ শুরুতেই যুক্ত করে নিয়েছে কেরাসকে। কেরাস দিয়ে ৯০%এর বেশি কাজ করা সম্ভব। \n",
        "\n",
        "আমরা পুরো জিনিসকে ৫টা ভাগে ভাগ করে ফেলেছি। \n",
        "<img src=\"https://raw.githubusercontent.com/raqueeb/TensorFlow2/master/assets/life.PNG\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "29ximwRWrFUZ"
      },
      "source": [
        "## মডেলের ধারণা \n",
        "\n",
        "এখন আমরা একটা মডেল তৈরি করতে চাই। মেশিন লার্নিং মডেল তবে জিনিসটা হবে খুবই সিম্প্লিস্টিক। একটা নিউরাল নেটওয়ার্ক। এখন কথা আসতে পারে নিউরাল নেটওয়ার্ক কি? (সামনে বিস্তারিত লিখেছি) নিউরন নেটওয়ার্ক আসলে কিছু অ্যালগরিদমের সেট যেটাকে কিছুটা তৈরি করা হয়েছে মানুষের মস্তিষ্কের নিউরাল এর ধারণার ওপর ভিত্তি করে। মানুষের মস্তিষ্কের সেই নিউরালকে আমরা নিয়ে এসেছি মেশিন লার্নিং এ। কারণ দুটোর কাজই এক। মেশিন লার্নিং এর যে নিউরাল নেটওয়ার্ক ব্যবহার করছি তার কাজ হচ্ছে বিভিন্ন লেয়ারে প্যাটার্ন ধরতে পারা। মানুষ যেমন তার হাজারো সেন্সর ডাটা আমাদের মস্তিষ্কে পাঠাচ্ছে, সেরকমভাবে নিউরাল নেটওয়ার্কের সবচেয়ে ছোট ইউনিট হচ্ছে একটা 'পারসেপ্ট্রন'। এই 'পারসেপ্ট্রন'গুলো যেভাবে প্যাটার্ন বুঝতে পারে তা সব সংখ্যার ভেক্টরে থাকে। আমাদের ইনপুট হিসেবে সেটা ছবি, শব্দ, লেখা অথবা টাইম সিরিজ হতে পারে - তবে সবকিছুকেই পাল্টে ফেলতে হবে সংখ্যায়।"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cayJ4SUurFUa"
      },
      "source": [
        "## বিভিন্ন লেয়ারে ফিচার এক্সট্রাকশন \n",
        "\n",
        "সাধারণত: নিউরাল নেটওয়ার্কগুলো আমাদেরকে ডেটাকে বিভিন্ন গ্রুপে ক্লাস্টার করে দেয়। আমাদের ইনপুট ডাটার ওপর ভিত্তি করে সেটার ক্লাসিফিকেশন/রিগ্রেশন হবে শেষ লেয়ারে। ধরা যাক আমাদের কাছে বেশ কিছু 'লেবেল ছাড়া' ডাটা আছে। নিউরাল নেটওয়ার্কগুলো এই লেবেল ছাড়া ডাটাগুলোকে তাদের মধ্যে বিভিন্ন মিল/সঙ্গতি/অসঙ্গতি দেখে সেগুলোকে আলাদা করে গ্রুপিং করে সে। এরপর তাকে একটা লেবেল সহ ডাটাসেট দিলে সেই ট্রেনিং থেকে ফিচারগুলোকে 'এক্সট্রাক্ট' করতে পারে। নিউরাল নেটওয়ার্ক এর একটা বড় কাজ হচ্ছে বিভিন্ন লেয়ারে বিভিন্ন ফিচার এক্সট্রাক্ট করে সে। (সামনে বিস্তারিত আলাপ করেছি) সবশেষে একটা মানুষকে যদি ক্লাসিফাই করতে হয়, মানে চিনতে হয়, তাহলে সেটা শুরু হবে সেই পিক্সেল থেকে যার পর মানুষের মুখের একেকটা ফিচারের কোনা, মানে নাক, মুখ বানানোর জন্য যা যা ফিচার লাগবে সেগুলোকে সে এক্সট্রাক্ট করবে তার নিচের লেয়ার থেকে।"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3XI4-3v7rFUb"
      },
      "source": [
        "## ছোট্ট একটা মডেল\n",
        "\n",
        "শুরুতেই একটু অংক। ডিপ লার্নিং একটা এন্ড টু এন্ড ইকোসিস্টেম, বিশেষ করে আমরা যখন টেন্সর-ফ্লো এনভায়রমেন্ট ব্যবহার করব। আমাদের এই নিউরাল নেটওয়ার্ক আউটপুটের সাথে ইনপুটের একটা চমৎকার ম্যাপিং করে। এর কাজ হচ্ছে ডাটার মধ্যে কোরিলেশন বের করা। অনেকে এইজন্য এর নাম বলে থাকেন 'ইউনিভার্সাল অ্যাপ্রক্সিমেটর'। কারণ এর কাজ হচ্ছে একটা অজানা ফাংশন f(x) = y এর ভ্যালুকে ধারণা করা, x আর y হচ্ছে তার ইনপুট এবং আউটপুট। আগের বইয়ে আলাপ করা হয়েছে। মেশিন লার্নিং এর এই শেখার প্রসেসে, নিউরাল নেটওয়ার্ক খুঁজে পায় তার আসল ফাংশন, যেটাকে আমরা বলতে পারি একটা প্রসেস যা x থেকে থেকে ইনপুট নিয়ে আপডেট করবে y কে। \n",
        "\n",
        "উদাহরণ কি দেব একটা? f(x) = 2x + 9 = y এর মত অসংখ্য উদাহরণ নিয়ে আলাপ করব সামনে।"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VM7_9Klvq7MO"
      },
      "source": [
        "## হাতেকলমে মডেল তৈরি \n",
        "\n",
        "যেহেতু আমরা একটা খুবই সহজ মডেল তৈরি করছি সেটাকে ডিপ লার্নিং এর ভাষায় আমরা বলছি ‘ডেন্স’ নেটওয়ার্ক। মানে একটার সাথে আরেকটার সরাসরি কানেকশন। গায়ে গায়ে লেগে থাকা। নেটওয়ার্কের একেকটা পারসেপ্ট্রন আরেকটার সাথে সম্পূর্ণভাবে কানেক্টেড থাকে। এর জন্যই একে বলা হচ্ছে 'ডেন্স'। 'ডেন্স'লি কানেক্টেড। (সামনে বিস্তারিত বলেছি) ‘ডেন্স’এর মডেল একেকটার সাথে স্ট্যাক করে কানেক্টেড মডেল। সেখানে আমরা ব্যবহার করবো \"সিকোয়েন্সিয়াল\", মানে একটার পর আরেকটা।\n",
        "\n",
        "নিউরাল নেটওয়ার্ক তৈরি করতে আমাদের এটার লেয়ারগুলোর কনফিগারেশন বলতে হবে। লেয়ার ঠিকমতো কনফিগার করা হলে আমরা মডেলকে কম্পাইল করব। আগের ছবিতেও দেখেছেন এই জিনিসটা। যেহেতু আমাদের সমস্যা একটা লিনিয়ার রিগ্রেশন এর মত, মানে একটা সরল রেখার ইকুয়েশনের মত, সে কারণে আমাদের নেটওয়ার্কে একটা লেয়ার প্রয়োজন। সঙ্গে একটা নিউরন। সামনে দরকার মতো আমরা আরো কিছু লেয়ার নিয়ে আলাপ করব।\n",
        "\n",
        "## লেয়ারের কনফিগারেশন\n",
        "\n",
        "একটা নিউরাল নেটওয়ার্কের বেসিক বিল্ডিং ব্লক হচ্ছে লেয়ার। লেয়ারগুলোতে যে ডেটা ফিড করানো হয় সেখান থেকে সে ডেটার রিপ্রেজেন্টেশনগুলোকে ঠিকমতো এক্সট্রাক্ট করে নিয়ে নেয় একেকটা লেয়ারে। বেশিরভাগ ডিপ লার্নিং মডেলগুলোর লেয়ার একটার সাথে আরেকটার কানেকশন ডেটাগুলোকে ঠিকমতো বুঝতে সাহায্য করে (ভিন্ন চ্যাপ্টার আছে সামনে)। যেহেতু আমাদের সমস্যাটা খুবই সহজ সে কারণে কিন্তু আমাদের এই নেটওয়ার্কে একটা লেয়ার হলেই চলবে। এই একটা লেয়ারে আমরা একটা নিউরন চালাবো।\n",
        "\n",
        "## তৈরি করি একটা লেয়ার\n",
        "\n",
        "শুরুতেই লেয়ারটার নাম দিয়ে দিচ্ছি `l0`। মনে আছে scikit-learn এর কথা? সেই একইভাবে আমরা টেন্সর-ফ্লো একটা এপিআই কল করব আমাদের এই 'ডেন্স' লেয়ারের জন্য। যেহেতু আমরা সরাসরি টেন্সর-ফ্লো এর সাথে কথা বলতে চাইবো না এই মুহূর্তে, অবশ্যই সেটা বেশ কমপ্লেক্স, তাই একটা হাই লেভেল হেল্পার এপিআই দিয়ে এক্সেস করব নিচের কঠিন টেন্সর-ফ্লোকে। এখন আমরা শুধু মনে রাখি 'কেরাস' হচ্ছে আমাদের সেই হাই-লেভেল এপিআই যা টেন্সর-ফ্লো এর কম্প্লেক্সিটি লুকিয়ে রাখে। সবচেয়ে বড় কথা হচ্ছে টেন্সর-ফ্লো ২.০ এর সঙ্গে ইন-বিল্ট এসেছে এই 'কেরাস এপিআই'।\n",
        "\n",
        "## একটা লেয়ার তৈরি করি \n",
        "\n",
        "আমাদের লেয়ার `l0` মানে লেয়ার জিরো। এটাকে তৈরি করছি `tf.keras.layers.Dense`কে ইন্সট্যান্সিয়েট করে। ইন্সট্যান্সিয়েট ব্যাপারটা আমরা আলাপ করেছি সাইকিট-লার্নের আগের বইতে। নিচের কনফিগারেশনগুলো দেখি।\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/raqueeb/TensorFlow2/master/assets/nn1.png\">\n",
        "\n",
        "এখানে একটা ইনপুট, একটা লেয়ার, একটা নিউরন, এবং একটা আউটপুট। ছবি দেখুন। ইনপুটে একটা ভ্যালু, input_shape=[1], এটা একটা ১ ডাইমেনশনের একটা সংখ্যা।\n",
        "\n",
        "*   `input_shape=[1]` — Input এর অর্থ হচ্ছে ইনপুট লেয়ার এ একটা ভ্যালু সে এক্সপেক্ট করছে। সিঙ্গেল ভ্যালু। সেই হিসেবে আমরা এটাকে বলছি এক ডাইমেনশনের একটা অ্যারে যার একটাই সদস্য। যেহেতু এটা এই মডেলের প্রথম এবং একটাই লেয়ার সে কারণে এই ইনপুট শেপ হচ্ছে পুরো মডেলিং ইনপুট শেপ। আমাদের এই সিঙ্গেল ভ্যালু হচ্ছে একটা ফ্লোটিং পয়েন্ট সংখ্যা, যা আসলে 15 সেকেন্ডে ঝিঁঝিঁপোকার ডাকের সংখ্যা।\n",
        "\n",
        "*   `units=1` — এই সংখ্যা দিয়ে আমরা বোঝাতে চাচ্ছি কতগুলো নিউরন হবে ওই লেয়ারে। আমাদের এই নিউরনের সংখ্যা বলে দেয় কতগুলো ইন্টারনাল ভ্যারিয়েবল সেই লেয়ারকে চেষ্টা করতে হবে শিখতে সমস্যাটা সমাধান করতে। সেটা নির্ভর করছে কতগুলো ইনপুট যুক্ত আছে সেই নিউরনের সাথে। মিতু এটাই এই মডেলের সব শেষ লেয়ার, আমাদের মডেলের আউটপুটের সাইজও কিন্তু ১। সারাদিন আউটপুট হচ্ছে একটা সিঙ্গেল ফ্লোট ভ্যালু ডিগ্রী সেলসিয়াস। ঝিঝি পোকা কত ডাক দিলে তার করেসপন্ডিং তাপমাত্রা। এটা যখন কয়েকটা লেয়ারের নেটওয়ার্ক হবে তাহলে সেই লেয়ারের সাইজ এবং সেপ একি হতে হবে পরবর্তী লেয়ারের input_shape।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pRllo2HLfXiu",
        "colab": {}
      },
      "source": [
        "l0 = tf.keras.layers.Dense(units=1, input_shape=[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_F00_J9duLBD"
      },
      "source": [
        "### লেয়ারগুলোকে মডেলে অ্যাসেম্বল করা\n",
        "\n",
        "যখন আমরা লেয়ারগুলোকে ডিফাইন করে ফেললাম এখন তাদেরকে মডেল এ যোগ করার পালা। আমরা যখন সিকুয়েন্সিয়াল মডেলকে আমাদের লেয়ারের আর্গুমেন্ট চেপে ধরে নেব তখন একটার পর আরেকটা লেয়ার কানেক্টেড থাকবে যেভাবে আমরা একটা থেকে আরেকটা লিস্ট করতে থাকবো। এখানে আমরা কেরাসের \"সিকোয়েন্সিয়াল\" ক্লাস ব্যবহার করছি। \n",
        "\n",
        "কোরাসে নিউরাল নেটওয়ার্কগুলো ডিফাইন করা থাকে লেয়ারের সিকোয়েন্স হিসেবে। এই লেয়ারের কনটেইনার হচ্ছে \"সিকোয়েন্সিয়াল\" ক্লাস। \n",
        "\n",
        "এই মডেলটার কিন্তু একটাই লেয়ার l0 ।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cSp-GpLSuMRq",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([l0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t7pfHfWxust0"
      },
      "source": [
        "**আরেক ভাবেও করা যায়**\n",
        "\n",
        "আমরা যদি আগে থেকে মডেলকে ডিফাইন করি, তাহলে লেয়ারগুলোকে ভেতরে ফেলা যায়। এটা একটা ভালো প্রোগ্রামিং স্টাইল। \n",
        "\n",
        "```python\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Dense(units=1, input_shape=[1])\n",
        "])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kiZG7uhm8qCF"
      },
      "source": [
        "## মডেল কম্পাইলেশন, সঙ্গে থাকছে লস এবং অপটিমাইজার ফাংশন\n",
        "\n",
        "ট্রেনিং এর আগে আমাদের মডেলকে কম্পাইল করে নিতে হবে। মডেলকে কম্পাইল করতে গেলে আমাদের নিচের দুটো ফাংশন কে ডিফাইন করতে হবে:\n",
        "\n",
        "- **লস ফাংশন** — এটা নিয়ে আমরা আগের বইতে আলাপ করেছিলাম। আমাদের যেটা আউটকাম আসার কথা সেখান থেকে প্রেডিকশন কত দূরে? আমাদের কাজ হচ্ছে এই দূরত্বটাকে ঠিকমতো মাপা। এই দুটোর মাঝখানে যে দূরত্ব সেটা কি আমরা লস বলছি। মনে আছে আমরা এর আগে কিভাবে 'মিন স্কোয়ারড এরর' বের করেছিলাম, আগের বইতে? অংকে।\n",
        "\n",
        "- **অপটিমাইজার ফাংশন** — আমাদের নিউরাল নেটওয়ার্কের যে ইন্টার্নাল ভ্যালুগুলো আছে সেগুলোকে কমিয়ে আনার জন্য এই ফাংশন। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m8YQN1H41L-Y",
        "colab": {}
      },
      "source": [
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer=tf.keras.optimizers.Adam(0.1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "17M3Pqv4P52R"
      },
      "source": [
        "এই ফাংশনগুলো সাধারণত ব্যবহার হয় ট্রেনিং এর সময়। দুটোর কাজ কিন্তু প্রাসঙ্গিক। প্রথমটার কাজ হচ্ছে প্রতিটা পয়েন্টে কত লস হচ্ছে সেটা বের করা, পরেরটা সেটাকে ধরে সেই লসকে কমিয়ে নিয়ে আসে। এই যে আমরা প্রতিটা পয়েন্টে লস ক্যালকুলেট করি, মানে ট্রেনিং ডেটার আসল ‘আউটকাম’ থেকে প্রেডিকশন কত দূরে আছে, আর সেই দুটোকে কমিয়ে নিয়ে আসতে যা কাজ করতে হয় এই দুটো ফাংশনকে - সেটাকে আমরা ট্রেনিং বলতে পারি।\n",
        "\n",
        "আমাদের এই ট্রেনিং এর সময় \"`model.fit()`\" ‘অপটিমাইজার’ ফাংশন মডেলের যে ইন্টারনাল ভ্যারিয়েবলগুলো (ওয়েট) আছে সেগুলোর মধ্যে দূরত্বকে কমিয়ে আনার জন্য যা যা এডজাস্টমেন্ট দরকার সেগুলো করে সে। সে ততক্ষণ এই কাজ করতে থাকে (ওই ইন্টারনাল ভেরিয়েবলগুলোর মধ্যে যা যা এডজাস্টমেন্ট দরকার) যতক্ষণ পর্যন্ত আমাদের মডেলের ভেতরের আসল ইকুয়েশনের সমান না হয়। আমাদের ইকুয়েশন হচ্ছে ১৫ সেকেন্ডে ঝিঝি পোকার ডাকের সংখ্যার সাথে ওই সময়ের তাপমাত্রার একটা সম্পর্ক বের করা। এটা বের করতে পারলে আমাদের কাজ হাসিল।\n",
        "\n",
        "আমরা এতক্ষণ যে ইন্টারনাল ভেরিয়েবলগুলোর কথা বললাম সেগুলো কিন্তু ইনপুট এর সাথে তার ‘করেসপন্ডিং’ ‘ওয়েট’। এগুলো নিয়ে আমরা নিউরাল নেটওয়ার্কের ‘পারসেপট্রন’ নিয়ে যখন আলাপ করব তখন বোঝা যাবে। আমাদের ডিপ লার্নিং ফ্রেমওয়ার্ক টেন্সর-ফ্লো এর ব্যাকএন্ডে কিছু বড় বড় অংকের অ্যানালাইসিস করে এই এডজাস্টমেন্ট টিউনিং করার জন্য। এর ব্যাকএন্ডে যে অংকটা আছে সেটাকে আমরা বলছি ‘গ্রেডিয়েন্ট ডিসেন্ট’। সেটা নিয়েও আলাপ করব সামনে।\n",
        "\n",
        "আপনি যদি ভালোভাবে দেখেন আমাদের এখানে যে লস ফাংশন ব্যবহার করেছি সেটা হচ্ছে ‘মিন স্কোয়ারড এরর’। সাইকিট-লার্ন বইটাতে এটা নিয়ে বেশ বড় একটা অংক করেছিলাম। পাশাপাশি ‘অপটিমাইজার’ এর ক্ষেত্রে ‘অ্যাডাম’ ব্যবহার করেছি যা আসলে এ ধরনের মডেলের জন্য ভালোভাবেই কাজ করে। এটা একটা লেটেস্ট ট্রেন্ড, সুন্দর কাজ করে। ‘অ্যাডাম’ মানে এডাপ্টিভ মোমেন্ট এস্টিমেশন। তবে এছাড়াও আমরা অন্যান্য প্যারামিটারগুলো নিয়েও সামনে আলাপ করব।\n",
        "\n",
        "‘অপটিমাইজার’ এর ভেতরের আরেকটা অংশ নিয়ে এখনই আলাপ করলে ব্যাপারটা সামনে সহজ হয়ে যাবে। এখানে দেখুন আমরা 0.1 যে সংখ্যাটা ব্যবহার করেছি, সেটা আসলে ‘লার্নিং রেট’। কি হারে মডেল শিখছে। এটা মডেলের ভেতরে যখন ইন্টারনাল ভেরিয়েবল বা ওয়েট নিজেদের মধ্যে এডজাস্ট করে সেটা একটা স্টেপ সাইজ ধরে করে। হাটিহাটি পা পা করে। পাহাড় থেকে নামার মতো। এই সংখ্যাটা যদি খুব কম হয় তাহলে একটা মডেল ট্রেইন করতে অনেক বেশি ‘আইটারেশন’ লাগবে। আবার সংখ্যাটা বড় হলে মডেলের অ্যাক্যুরেসি কমে যাবে। তাহলে মধ্য়পন্থা। হাতে কলমের একটা বড় মজা হচ্ছে হাইপার প্যারামিটারগুলোকে টিউন করার জন্য আমাদের ভ্যালুগুলো দিয়ে কিছুটা ট্রায়াল দিতে হবে। তবে ইন্ডাস্ট্রি স্ট্যান্ডার্ড অনুযায়ী এটার একটা ডিফল্ট ভ্যালু আছে যা ০.০০১ থেকে ০.১ পর্যন্ত ভালো কাজ করে।"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c-Jk4dG91dvD"
      },
      "source": [
        "## মডেলের ট্রেনিং\n",
        "\n",
        "Scikit-learn এর মত এখানেও আমরা মডেলকে ট্রেইন করবো ফিট \"`fit`\" মেথডকে কল করে। ট্রেনিং এর সময় আমাদের মডেল ১৫ সেকেন্ডের ঝিঁঝিঁ পোকার ডাক এর সংখ্যার সাথে বর্তমান যে ইন্টার্নাল ভেরিয়েবলগুলো আছে (যাকে আমরা বলছি ওয়েট) তাদেরকে ব্যবহার করে এবং তার আউটপুট ভ্যালু যেটাকে আমরা বলছি বর্তমান তাপমাত্রা - এ দুটোর মধ্যে একটা ক্যালকুলেশন করে এদের মধ্যে রিলেশনশিপ এডজাস্ট করতে থাকে। যেহেতু শুরুতেই এই ওয়েটগুলোকে দৈব চয়নের ভিত্তিতে সেট করা হয় সে কারণে শুরুর দিকে তাদের আউটপুট আসল ভ্যালুর কাছাকাছি না আসার সম্ভাবনা বেশি। সে কারণে আমাদের যে আসল আউটপুট (ট্রেনিং ডেটা থেকে) আর যে আউটপুটটা এখন ক্যালকুলেট করা হলো লস ফাংশন দিয়ে, সেটাকে অপটিমাইজার ফাংশন পই পই করে বলে দেয় কিভাবে ওয়েটগুলোকে এডজাস্ট করবে সামনে।\n",
        "\n",
        "এই পুরো ব্যাপারটা মানে পুরো সাইকেলকে আমরা বলছি ১. একটা ক্যালকুলেশন, ২. তার সঙ্গে মিলিয়ে দেখা, ৩. এর পাশাপাশি ওয়েটগুলোর যে এডজাস্টমেন্ট এই তিনটা জিনিসকে ম্যানেজ করে আমাদের এই ফিট মেথড। Scikit-learn এর মত একই ধরনের আর্গুমেন্ট তবে সঙ্গে আরো কয়েকটা এলিমেন্ট এসেছে নতুন করে। আমাদের প্রথম আর্গুমেন্ট হচ্ছে ইনপুট আর পরের আর্গুমেন্টটা হচ্ছে আমরা যেটা পেতে চাই। একদম scikit-learn। এরপরের আর্গুমেন্ট হচ্ছে ইপক (epochs), মানে পুরো ট্রেনিং ডাটা কতবার পুরোপুরি আগা থেকে গোড়া পর্যন্ত চালাবে, শেষে হচ্ছে ভার্বস (verbose) আর্গুমেন্ট যেটা নির্ধারণ করে আমাদের আউটপুটে বাড়তি ইনফরমেশন দিবে কি দিবেনা।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lpRrl7WK10Pq",
        "outputId": "7441c135-83fe-48bc-c9d4-a6f4504ac719",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# X = chips_15s\n",
        "# y = temp_celsius\n",
        "\n",
        "# history = model.fit(chips_15s, temp_celsius, epochs=500, verbose=True)\n",
        "history = model.fit(X, y, epochs=500, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "55/55 [==============================] - 0s 2ms/sample - loss: 242.3474\n",
            "Epoch 2/500\n",
            "55/55 [==============================] - 0s 188us/sample - loss: 90.5471\n",
            "Epoch 3/500\n",
            "55/55 [==============================] - 0s 92us/sample - loss: 17.2494\n",
            "Epoch 4/500\n",
            "55/55 [==============================] - 0s 81us/sample - loss: 6.1930\n",
            "Epoch 5/500\n",
            "55/55 [==============================] - 0s 81us/sample - loss: 26.9660\n",
            "Epoch 6/500\n",
            "55/55 [==============================] - 0s 86us/sample - loss: 44.1277\n",
            "Epoch 7/500\n",
            "55/55 [==============================] - 0s 80us/sample - loss: 43.1445\n",
            "Epoch 8/500\n",
            "55/55 [==============================] - 0s 81us/sample - loss: 28.6341\n",
            "Epoch 9/500\n",
            "55/55 [==============================] - 0s 85us/sample - loss: 12.7491\n",
            "Epoch 10/500\n",
            "55/55 [==============================] - 0s 156us/sample - loss: 4.1697\n",
            "Epoch 11/500\n",
            "55/55 [==============================] - 0s 144us/sample - loss: 4.0319\n",
            "Epoch 12/500\n",
            "55/55 [==============================] - 0s 112us/sample - loss: 8.9887\n",
            "Epoch 13/500\n",
            "55/55 [==============================] - 0s 86us/sample - loss: 12.7106\n",
            "Epoch 14/500\n",
            "55/55 [==============================] - 0s 85us/sample - loss: 12.3777\n",
            "Epoch 15/500\n",
            "55/55 [==============================] - 0s 90us/sample - loss: 8.2733\n",
            "Epoch 16/500\n",
            "55/55 [==============================] - 0s 76us/sample - loss: 4.6295\n",
            "Epoch 17/500\n",
            "55/55 [==============================] - 0s 90us/sample - loss: 2.7742\n",
            "Epoch 18/500\n",
            "55/55 [==============================] - 0s 64us/sample - loss: 3.4202\n",
            "Epoch 19/500\n",
            "55/55 [==============================] - 0s 96us/sample - loss: 4.9119\n",
            "Epoch 20/500\n",
            "55/55 [==============================] - 0s 96us/sample - loss: 5.4869\n",
            "Epoch 21/500\n",
            "55/55 [==============================] - 0s 102us/sample - loss: 4.8414\n",
            "Epoch 22/500\n",
            "55/55 [==============================] - 0s 101us/sample - loss: 3.5867\n",
            "Epoch 23/500\n",
            "55/55 [==============================] - 0s 85us/sample - loss: 2.8983\n",
            "Epoch 24/500\n",
            "55/55 [==============================] - 0s 83us/sample - loss: 2.7156\n",
            "Epoch 25/500\n",
            "55/55 [==============================] - 0s 85us/sample - loss: 3.0862\n",
            "Epoch 26/500\n",
            "55/55 [==============================] - 0s 84us/sample - loss: 3.3366\n",
            "Epoch 27/500\n",
            "55/55 [==============================] - 0s 83us/sample - loss: 3.2238\n",
            "Epoch 28/500\n",
            "55/55 [==============================] - 0s 80us/sample - loss: 2.8616\n",
            "Epoch 29/500\n",
            "55/55 [==============================] - 0s 108us/sample - loss: 2.5809\n",
            "Epoch 30/500\n",
            "55/55 [==============================] - 0s 88us/sample - loss: 2.6131\n",
            "Epoch 31/500\n",
            "55/55 [==============================] - 0s 91us/sample - loss: 2.7283\n",
            "Epoch 32/500\n",
            "55/55 [==============================] - 0s 103us/sample - loss: 2.7822\n",
            "Epoch 33/500\n",
            "55/55 [==============================] - 0s 121us/sample - loss: 2.7199\n",
            "Epoch 34/500\n",
            "55/55 [==============================] - 0s 107us/sample - loss: 2.5755\n",
            "Epoch 35/500\n",
            "55/55 [==============================] - 0s 102us/sample - loss: 2.4862\n",
            "Epoch 36/500\n",
            "55/55 [==============================] - 0s 143us/sample - loss: 2.5076\n",
            "Epoch 37/500\n",
            "55/55 [==============================] - 0s 112us/sample - loss: 2.5278\n",
            "Epoch 38/500\n",
            "55/55 [==============================] - 0s 94us/sample - loss: 2.5260\n",
            "Epoch 39/500\n",
            "55/55 [==============================] - 0s 96us/sample - loss: 2.4924\n",
            "Epoch 40/500\n",
            "55/55 [==============================] - 0s 109us/sample - loss: 2.4330\n",
            "Epoch 41/500\n",
            "55/55 [==============================] - 0s 106us/sample - loss: 2.4192\n",
            "Epoch 42/500\n",
            "55/55 [==============================] - 0s 107us/sample - loss: 2.4170\n",
            "Epoch 43/500\n",
            "55/55 [==============================] - 0s 84us/sample - loss: 2.3989\n",
            "Epoch 44/500\n",
            "55/55 [==============================] - 0s 129us/sample - loss: 2.3630\n",
            "Epoch 45/500\n",
            "55/55 [==============================] - 0s 111us/sample - loss: 2.3389\n",
            "Epoch 46/500\n",
            "55/55 [==============================] - 0s 113us/sample - loss: 2.3235\n",
            "Epoch 47/500\n",
            "55/55 [==============================] - 0s 112us/sample - loss: 2.3133\n",
            "Epoch 48/500\n",
            "55/55 [==============================] - 0s 101us/sample - loss: 2.2974\n",
            "Epoch 49/500\n",
            "55/55 [==============================] - 0s 94us/sample - loss: 2.2820\n",
            "Epoch 50/500\n",
            "55/55 [==============================] - 0s 99us/sample - loss: 2.2567\n",
            "Epoch 51/500\n",
            "55/55 [==============================] - 0s 115us/sample - loss: 2.2425\n",
            "Epoch 52/500\n",
            "55/55 [==============================] - 0s 107us/sample - loss: 2.2249\n",
            "Epoch 53/500\n",
            "55/55 [==============================] - 0s 114us/sample - loss: 2.2097\n",
            "Epoch 54/500\n",
            "55/55 [==============================] - 0s 103us/sample - loss: 2.1919\n",
            "Epoch 55/500\n",
            "55/55 [==============================] - 0s 101us/sample - loss: 2.1790\n",
            "Epoch 56/500\n",
            "55/55 [==============================] - 0s 103us/sample - loss: 2.1643\n",
            "Epoch 57/500\n",
            "55/55 [==============================] - 0s 107us/sample - loss: 2.1455\n",
            "Epoch 58/500\n",
            "55/55 [==============================] - 0s 119us/sample - loss: 2.1263\n",
            "Epoch 59/500\n",
            "55/55 [==============================] - 0s 104us/sample - loss: 2.1244\n",
            "Epoch 60/500\n",
            "55/55 [==============================] - 0s 121us/sample - loss: 2.1185\n",
            "Epoch 61/500\n",
            "55/55 [==============================] - 0s 136us/sample - loss: 2.0834\n",
            "Epoch 62/500\n",
            "55/55 [==============================] - 0s 108us/sample - loss: 2.0602\n",
            "Epoch 63/500\n",
            "55/55 [==============================] - 0s 95us/sample - loss: 2.0561\n",
            "Epoch 64/500\n",
            "55/55 [==============================] - 0s 121us/sample - loss: 2.0338\n",
            "Epoch 65/500\n",
            "55/55 [==============================] - 0s 103us/sample - loss: 2.0103\n",
            "Epoch 66/500\n",
            "55/55 [==============================] - 0s 111us/sample - loss: 1.9959\n",
            "Epoch 67/500\n",
            "55/55 [==============================] - 0s 76us/sample - loss: 1.9801\n",
            "Epoch 68/500\n",
            "55/55 [==============================] - 0s 145us/sample - loss: 1.9641\n",
            "Epoch 69/500\n",
            "55/55 [==============================] - 0s 124us/sample - loss: 1.9482\n",
            "Epoch 70/500\n",
            "55/55 [==============================] - 0s 118us/sample - loss: 1.9353\n",
            "Epoch 71/500\n",
            "55/55 [==============================] - 0s 129us/sample - loss: 1.9192\n",
            "Epoch 72/500\n",
            "55/55 [==============================] - 0s 110us/sample - loss: 1.9025\n",
            "Epoch 73/500\n",
            "55/55 [==============================] - 0s 116us/sample - loss: 1.8895\n",
            "Epoch 74/500\n",
            "55/55 [==============================] - 0s 126us/sample - loss: 1.8730\n",
            "Epoch 75/500\n",
            "55/55 [==============================] - 0s 119us/sample - loss: 1.8577\n",
            "Epoch 76/500\n",
            "55/55 [==============================] - 0s 117us/sample - loss: 1.8486\n",
            "Epoch 77/500\n",
            "55/55 [==============================] - 0s 114us/sample - loss: 1.8287\n",
            "Epoch 78/500\n",
            "55/55 [==============================] - 0s 106us/sample - loss: 1.8265\n",
            "Epoch 79/500\n",
            "55/55 [==============================] - 0s 115us/sample - loss: 1.8030\n",
            "Epoch 80/500\n",
            "55/55 [==============================] - 0s 118us/sample - loss: 1.7866\n",
            "Epoch 81/500\n",
            "55/55 [==============================] - 0s 127us/sample - loss: 1.7662\n",
            "Epoch 82/500\n",
            "55/55 [==============================] - 0s 118us/sample - loss: 1.7498\n",
            "Epoch 83/500\n",
            "55/55 [==============================] - 0s 122us/sample - loss: 1.7570\n",
            "Epoch 84/500\n",
            "55/55 [==============================] - 0s 120us/sample - loss: 1.7436\n",
            "Epoch 85/500\n",
            "55/55 [==============================] - 0s 136us/sample - loss: 1.7333\n",
            "Epoch 86/500\n",
            "55/55 [==============================] - 0s 124us/sample - loss: 1.6965\n",
            "Epoch 87/500\n",
            "55/55 [==============================] - 0s 111us/sample - loss: 1.6949\n",
            "Epoch 88/500\n",
            "55/55 [==============================] - 0s 134us/sample - loss: 1.6721\n",
            "Epoch 89/500\n",
            "55/55 [==============================] - 0s 102us/sample - loss: 1.6602\n",
            "Epoch 90/500\n",
            "55/55 [==============================] - 0s 96us/sample - loss: 1.6500\n",
            "Epoch 91/500\n",
            "55/55 [==============================] - 0s 87us/sample - loss: 1.6440\n",
            "Epoch 92/500\n",
            "55/55 [==============================] - 0s 69us/sample - loss: 1.6165\n",
            "Epoch 93/500\n",
            "55/55 [==============================] - 0s 106us/sample - loss: 1.6043\n",
            "Epoch 94/500\n",
            "55/55 [==============================] - 0s 125us/sample - loss: 1.6006\n",
            "Epoch 95/500\n",
            "55/55 [==============================] - 0s 126us/sample - loss: 1.5851\n",
            "Epoch 96/500\n",
            "55/55 [==============================] - 0s 118us/sample - loss: 1.5737\n",
            "Epoch 97/500\n",
            "55/55 [==============================] - 0s 120us/sample - loss: 1.5636\n",
            "Epoch 98/500\n",
            "55/55 [==============================] - 0s 129us/sample - loss: 1.5397\n",
            "Epoch 99/500\n",
            "55/55 [==============================] - 0s 116us/sample - loss: 1.5271\n",
            "Epoch 100/500\n",
            "55/55 [==============================] - 0s 105us/sample - loss: 1.5150\n",
            "Epoch 101/500\n",
            "55/55 [==============================] - 0s 116us/sample - loss: 1.5039\n",
            "Epoch 102/500\n",
            "55/55 [==============================] - 0s 313us/sample - loss: 1.4947\n",
            "Epoch 103/500\n",
            "55/55 [==============================] - 0s 124us/sample - loss: 1.4798\n",
            "Epoch 104/500\n",
            "55/55 [==============================] - 0s 117us/sample - loss: 1.4786\n",
            "Epoch 105/500\n",
            "55/55 [==============================] - 0s 92us/sample - loss: 1.4745\n",
            "Epoch 106/500\n",
            "55/55 [==============================] - 0s 60us/sample - loss: 1.4532\n",
            "Epoch 107/500\n",
            "55/55 [==============================] - 0s 118us/sample - loss: 1.4334\n",
            "Epoch 108/500\n",
            "55/55 [==============================] - 0s 107us/sample - loss: 1.4231\n",
            "Epoch 109/500\n",
            "55/55 [==============================] - 0s 102us/sample - loss: 1.4105\n",
            "Epoch 110/500\n",
            "55/55 [==============================] - 0s 104us/sample - loss: 1.4064\n",
            "Epoch 111/500\n",
            "55/55 [==============================] - 0s 108us/sample - loss: 1.3910\n",
            "Epoch 112/500\n",
            "55/55 [==============================] - 0s 97us/sample - loss: 1.3854\n",
            "Epoch 113/500\n",
            "55/55 [==============================] - 0s 98us/sample - loss: 1.3719\n",
            "Epoch 114/500\n",
            "55/55 [==============================] - 0s 106us/sample - loss: 1.3685\n",
            "Epoch 115/500\n",
            "55/55 [==============================] - 0s 124us/sample - loss: 1.3622\n",
            "Epoch 116/500\n",
            "55/55 [==============================] - 0s 135us/sample - loss: 1.3440\n",
            "Epoch 117/500\n",
            "55/55 [==============================] - 0s 110us/sample - loss: 1.3324\n",
            "Epoch 118/500\n",
            "55/55 [==============================] - 0s 107us/sample - loss: 1.3261\n",
            "Epoch 119/500\n",
            "55/55 [==============================] - 0s 105us/sample - loss: 1.3324\n",
            "Epoch 120/500\n",
            "55/55 [==============================] - 0s 105us/sample - loss: 1.2980\n",
            "Epoch 121/500\n",
            "55/55 [==============================] - 0s 120us/sample - loss: 1.3021\n",
            "Epoch 122/500\n",
            "55/55 [==============================] - 0s 118us/sample - loss: 1.2950\n",
            "Epoch 123/500\n",
            "55/55 [==============================] - 0s 120us/sample - loss: 1.2721\n",
            "Epoch 124/500\n",
            "55/55 [==============================] - 0s 107us/sample - loss: 1.2690\n",
            "Epoch 125/500\n",
            "55/55 [==============================] - 0s 113us/sample - loss: 1.2754\n",
            "Epoch 126/500\n",
            "55/55 [==============================] - 0s 109us/sample - loss: 1.2926\n",
            "Epoch 127/500\n",
            "55/55 [==============================] - 0s 126us/sample - loss: 1.2417\n",
            "Epoch 128/500\n",
            "55/55 [==============================] - 0s 138us/sample - loss: 1.2259\n",
            "Epoch 129/500\n",
            "55/55 [==============================] - 0s 132us/sample - loss: 1.2386\n",
            "Epoch 130/500\n",
            "55/55 [==============================] - 0s 102us/sample - loss: 1.2173\n",
            "Epoch 131/500\n",
            "55/55 [==============================] - 0s 102us/sample - loss: 1.2004\n",
            "Epoch 132/500\n",
            "55/55 [==============================] - 0s 95us/sample - loss: 1.1981\n",
            "Epoch 133/500\n",
            "55/55 [==============================] - 0s 122us/sample - loss: 1.1981\n",
            "Epoch 134/500\n",
            "55/55 [==============================] - 0s 128us/sample - loss: 1.1846\n",
            "Epoch 135/500\n",
            "55/55 [==============================] - 0s 127us/sample - loss: 1.1687\n",
            "Epoch 136/500\n",
            "55/55 [==============================] - 0s 126us/sample - loss: 1.1688\n",
            "Epoch 137/500\n",
            "55/55 [==============================] - 0s 115us/sample - loss: 1.1687\n",
            "Epoch 138/500\n",
            "55/55 [==============================] - 0s 116us/sample - loss: 1.1518\n",
            "Epoch 139/500\n",
            "55/55 [==============================] - 0s 154us/sample - loss: 1.1399\n",
            "Epoch 140/500\n",
            "55/55 [==============================] - 0s 130us/sample - loss: 1.1413\n",
            "Epoch 141/500\n",
            "55/55 [==============================] - 0s 73us/sample - loss: 1.1436\n",
            "Epoch 142/500\n",
            "55/55 [==============================] - 0s 81us/sample - loss: 1.1313\n",
            "Epoch 143/500\n",
            "55/55 [==============================] - 0s 113us/sample - loss: 1.1127\n",
            "Epoch 144/500\n",
            "55/55 [==============================] - 0s 100us/sample - loss: 1.1109\n",
            "Epoch 145/500\n",
            "55/55 [==============================] - 0s 98us/sample - loss: 1.1049\n",
            "Epoch 146/500\n",
            "55/55 [==============================] - 0s 94us/sample - loss: 1.0955\n",
            "Epoch 147/500\n",
            "55/55 [==============================] - 0s 103us/sample - loss: 1.0959\n",
            "Epoch 148/500\n",
            "55/55 [==============================] - 0s 126us/sample - loss: 1.0833\n",
            "Epoch 149/500\n",
            "55/55 [==============================] - 0s 112us/sample - loss: 1.0884\n",
            "Epoch 150/500\n",
            "55/55 [==============================] - 0s 108us/sample - loss: 1.0873\n",
            "Epoch 151/500\n",
            "55/55 [==============================] - 0s 93us/sample - loss: 1.0634\n",
            "Epoch 152/500\n",
            "55/55 [==============================] - 0s 95us/sample - loss: 1.0576\n",
            "Epoch 153/500\n",
            "55/55 [==============================] - 0s 108us/sample - loss: 1.0545\n",
            "Epoch 154/500\n",
            "55/55 [==============================] - 0s 111us/sample - loss: 1.0488\n",
            "Epoch 155/500\n",
            "55/55 [==============================] - 0s 86us/sample - loss: 1.0494\n",
            "Epoch 156/500\n",
            "55/55 [==============================] - 0s 93us/sample - loss: 1.0345\n",
            "Epoch 157/500\n",
            "55/55 [==============================] - 0s 103us/sample - loss: 1.0289\n",
            "Epoch 158/500\n",
            "55/55 [==============================] - 0s 123us/sample - loss: 1.0244\n",
            "Epoch 159/500\n",
            "55/55 [==============================] - 0s 124us/sample - loss: 1.0331\n",
            "Epoch 160/500\n",
            "55/55 [==============================] - 0s 108us/sample - loss: 1.0128\n",
            "Epoch 161/500\n",
            "55/55 [==============================] - 0s 107us/sample - loss: 1.0181\n",
            "Epoch 162/500\n",
            "55/55 [==============================] - 0s 116us/sample - loss: 1.0069\n",
            "Epoch 163/500\n",
            "55/55 [==============================] - 0s 131us/sample - loss: 1.0008\n",
            "Epoch 164/500\n",
            "55/55 [==============================] - 0s 99us/sample - loss: 0.9975\n",
            "Epoch 165/500\n",
            "55/55 [==============================] - 0s 102us/sample - loss: 0.9979\n",
            "Epoch 166/500\n",
            "55/55 [==============================] - 0s 115us/sample - loss: 1.0048\n",
            "Epoch 167/500\n",
            "55/55 [==============================] - 0s 87us/sample - loss: 0.9831\n",
            "Epoch 168/500\n",
            "55/55 [==============================] - 0s 82us/sample - loss: 0.9746\n",
            "Epoch 169/500\n",
            "55/55 [==============================] - 0s 82us/sample - loss: 0.9814\n",
            "Epoch 170/500\n",
            "55/55 [==============================] - 0s 97us/sample - loss: 0.9687\n",
            "Epoch 171/500\n",
            "55/55 [==============================] - 0s 186us/sample - loss: 0.9670\n",
            "Epoch 172/500\n",
            "55/55 [==============================] - 0s 88us/sample - loss: 0.9622\n",
            "Epoch 173/500\n",
            "55/55 [==============================] - 0s 108us/sample - loss: 0.9503\n",
            "Epoch 174/500\n",
            "55/55 [==============================] - 0s 141us/sample - loss: 0.9500\n",
            "Epoch 175/500\n",
            "55/55 [==============================] - 0s 102us/sample - loss: 0.9583\n",
            "Epoch 176/500\n",
            "55/55 [==============================] - 0s 96us/sample - loss: 0.9464\n",
            "Epoch 177/500\n",
            "55/55 [==============================] - 0s 99us/sample - loss: 0.9383\n",
            "Epoch 178/500\n",
            "55/55 [==============================] - 0s 108us/sample - loss: 0.9389\n",
            "Epoch 179/500\n",
            "55/55 [==============================] - 0s 131us/sample - loss: 0.9395\n",
            "Epoch 180/500\n",
            "55/55 [==============================] - 0s 98us/sample - loss: 0.9455\n",
            "Epoch 181/500\n",
            "55/55 [==============================] - 0s 96us/sample - loss: 0.9272\n",
            "Epoch 182/500\n",
            "55/55 [==============================] - 0s 113us/sample - loss: 0.9227\n",
            "Epoch 183/500\n",
            "55/55 [==============================] - 0s 118us/sample - loss: 0.9143\n",
            "Epoch 184/500\n",
            "55/55 [==============================] - 0s 104us/sample - loss: 0.9130\n",
            "Epoch 185/500\n",
            "55/55 [==============================] - 0s 103us/sample - loss: 0.9235\n",
            "Epoch 186/500\n",
            "55/55 [==============================] - 0s 132us/sample - loss: 0.9134\n",
            "Epoch 187/500\n",
            "55/55 [==============================] - 0s 107us/sample - loss: 0.9068\n",
            "Epoch 188/500\n",
            "55/55 [==============================] - 0s 119us/sample - loss: 0.9119\n",
            "Epoch 189/500\n",
            "55/55 [==============================] - 0s 117us/sample - loss: 0.8947\n",
            "Epoch 190/500\n",
            "55/55 [==============================] - 0s 98us/sample - loss: 0.9137\n",
            "Epoch 191/500\n",
            "55/55 [==============================] - 0s 104us/sample - loss: 0.9113\n",
            "Epoch 192/500\n",
            "55/55 [==============================] - 0s 85us/sample - loss: 0.9007\n",
            "Epoch 193/500\n",
            "55/55 [==============================] - 0s 117us/sample - loss: 0.8876\n",
            "Epoch 194/500\n",
            "55/55 [==============================] - 0s 91us/sample - loss: 0.9105\n",
            "Epoch 195/500\n",
            "55/55 [==============================] - 0s 133us/sample - loss: 0.8808\n",
            "Epoch 196/500\n",
            "55/55 [==============================] - 0s 129us/sample - loss: 0.8955\n",
            "Epoch 197/500\n",
            "55/55 [==============================] - 0s 119us/sample - loss: 0.8947\n",
            "Epoch 198/500\n",
            "55/55 [==============================] - 0s 116us/sample - loss: 0.8800\n",
            "Epoch 199/500\n",
            "55/55 [==============================] - 0s 117us/sample - loss: 0.8786\n",
            "Epoch 200/500\n",
            "55/55 [==============================] - 0s 84us/sample - loss: 0.8779\n",
            "Epoch 201/500\n",
            "55/55 [==============================] - 0s 101us/sample - loss: 0.8650\n",
            "Epoch 202/500\n",
            "55/55 [==============================] - 0s 93us/sample - loss: 0.8631\n",
            "Epoch 203/500\n",
            "55/55 [==============================] - 0s 94us/sample - loss: 0.8710\n",
            "Epoch 204/500\n",
            "55/55 [==============================] - 0s 111us/sample - loss: 0.8649\n",
            "Epoch 205/500\n",
            "55/55 [==============================] - 0s 101us/sample - loss: 0.8561\n",
            "Epoch 206/500\n",
            "55/55 [==============================] - 0s 115us/sample - loss: 0.8660\n",
            "Epoch 207/500\n",
            "55/55 [==============================] - 0s 98us/sample - loss: 0.8662\n",
            "Epoch 208/500\n",
            "55/55 [==============================] - 0s 92us/sample - loss: 0.8517\n",
            "Epoch 209/500\n",
            "55/55 [==============================] - 0s 93us/sample - loss: 0.8505\n",
            "Epoch 210/500\n",
            "55/55 [==============================] - 0s 81us/sample - loss: 0.8503\n",
            "Epoch 211/500\n",
            "55/55 [==============================] - 0s 126us/sample - loss: 0.8473\n",
            "Epoch 212/500\n",
            "55/55 [==============================] - 0s 116us/sample - loss: 0.8420\n",
            "Epoch 213/500\n",
            "55/55 [==============================] - 0s 96us/sample - loss: 0.8417\n",
            "Epoch 214/500\n",
            "55/55 [==============================] - 0s 103us/sample - loss: 0.8429\n",
            "Epoch 215/500\n",
            "55/55 [==============================] - 0s 105us/sample - loss: 0.8427\n",
            "Epoch 216/500\n",
            "55/55 [==============================] - 0s 125us/sample - loss: 0.8355\n",
            "Epoch 217/500\n",
            "55/55 [==============================] - 0s 112us/sample - loss: 0.8368\n",
            "Epoch 218/500\n",
            "55/55 [==============================] - 0s 117us/sample - loss: 0.8533\n",
            "Epoch 219/500\n",
            "55/55 [==============================] - 0s 114us/sample - loss: 0.8366\n",
            "Epoch 220/500\n",
            "55/55 [==============================] - 0s 111us/sample - loss: 0.8384\n",
            "Epoch 221/500\n",
            "55/55 [==============================] - 0s 123us/sample - loss: 0.8385\n",
            "Epoch 222/500\n",
            "55/55 [==============================] - 0s 107us/sample - loss: 0.8281\n",
            "Epoch 223/500\n",
            "55/55 [==============================] - 0s 104us/sample - loss: 0.8269\n",
            "Epoch 224/500\n",
            "55/55 [==============================] - 0s 112us/sample - loss: 0.8233\n",
            "Epoch 225/500\n",
            "55/55 [==============================] - 0s 113us/sample - loss: 0.8234\n",
            "Epoch 226/500\n",
            "55/55 [==============================] - 0s 104us/sample - loss: 0.8376\n",
            "Epoch 227/500\n",
            "55/55 [==============================] - 0s 106us/sample - loss: 0.8362\n",
            "Epoch 228/500\n",
            "55/55 [==============================] - 0s 107us/sample - loss: 0.8175\n",
            "Epoch 229/500\n",
            "55/55 [==============================] - 0s 114us/sample - loss: 0.8186\n",
            "Epoch 230/500\n",
            "55/55 [==============================] - 0s 116us/sample - loss: 0.8190\n",
            "Epoch 231/500\n",
            "55/55 [==============================] - 0s 120us/sample - loss: 0.8185\n",
            "Epoch 232/500\n",
            "55/55 [==============================] - 0s 155us/sample - loss: 0.8250\n",
            "Epoch 233/500\n",
            "55/55 [==============================] - 0s 134us/sample - loss: 0.8239\n",
            "Epoch 234/500\n",
            "55/55 [==============================] - 0s 134us/sample - loss: 0.8137\n",
            "Epoch 235/500\n",
            "55/55 [==============================] - 0s 123us/sample - loss: 0.8211\n",
            "Epoch 236/500\n",
            "55/55 [==============================] - 0s 141us/sample - loss: 0.8284\n",
            "Epoch 237/500\n",
            "55/55 [==============================] - 0s 125us/sample - loss: 0.8155\n",
            "Epoch 238/500\n",
            "55/55 [==============================] - 0s 120us/sample - loss: 0.8130\n",
            "Epoch 239/500\n",
            "55/55 [==============================] - 0s 117us/sample - loss: 0.8060\n",
            "Epoch 240/500\n",
            "55/55 [==============================] - 0s 97us/sample - loss: 0.8337\n",
            "Epoch 241/500\n",
            "55/55 [==============================] - 0s 265us/sample - loss: 0.8114\n",
            "Epoch 242/500\n",
            "55/55 [==============================] - 0s 125us/sample - loss: 0.8118\n",
            "Epoch 243/500\n",
            "55/55 [==============================] - 0s 115us/sample - loss: 0.8236\n",
            "Epoch 244/500\n",
            "55/55 [==============================] - 0s 112us/sample - loss: 0.8026\n",
            "Epoch 245/500\n",
            "55/55 [==============================] - 0s 103us/sample - loss: 0.8030\n",
            "Epoch 246/500\n",
            "55/55 [==============================] - 0s 113us/sample - loss: 0.8289\n",
            "Epoch 247/500\n",
            "55/55 [==============================] - 0s 122us/sample - loss: 0.8114\n",
            "Epoch 248/500\n",
            "55/55 [==============================] - 0s 113us/sample - loss: 0.7999\n",
            "Epoch 249/500\n",
            "55/55 [==============================] - 0s 115us/sample - loss: 0.8249\n",
            "Epoch 250/500\n",
            "55/55 [==============================] - 0s 135us/sample - loss: 0.8017\n",
            "Epoch 251/500\n",
            "55/55 [==============================] - 0s 137us/sample - loss: 0.8000\n",
            "Epoch 252/500\n",
            "55/55 [==============================] - 0s 145us/sample - loss: 0.8164\n",
            "Epoch 253/500\n",
            "55/55 [==============================] - 0s 138us/sample - loss: 0.8054\n",
            "Epoch 254/500\n",
            "55/55 [==============================] - 0s 129us/sample - loss: 0.7903\n",
            "Epoch 255/500\n",
            "55/55 [==============================] - 0s 137us/sample - loss: 0.7995\n",
            "Epoch 256/500\n",
            "55/55 [==============================] - 0s 125us/sample - loss: 0.8066\n",
            "Epoch 257/500\n",
            "55/55 [==============================] - 0s 128us/sample - loss: 0.7899\n",
            "Epoch 258/500\n",
            "55/55 [==============================] - 0s 120us/sample - loss: 0.7985\n",
            "Epoch 259/500\n",
            "55/55 [==============================] - 0s 125us/sample - loss: 0.8217\n",
            "Epoch 260/500\n",
            "55/55 [==============================] - 0s 121us/sample - loss: 0.7952\n",
            "Epoch 261/500\n",
            "55/55 [==============================] - 0s 159us/sample - loss: 0.7888\n",
            "Epoch 262/500\n",
            "55/55 [==============================] - 0s 117us/sample - loss: 0.7910\n",
            "Epoch 263/500\n",
            "55/55 [==============================] - 0s 124us/sample - loss: 0.8078\n",
            "Epoch 264/500\n",
            "55/55 [==============================] - 0s 110us/sample - loss: 0.7964\n",
            "Epoch 265/500\n",
            "55/55 [==============================] - 0s 138us/sample - loss: 0.7897\n",
            "Epoch 266/500\n",
            "55/55 [==============================] - 0s 131us/sample - loss: 0.8332\n",
            "Epoch 267/500\n",
            "55/55 [==============================] - 0s 136us/sample - loss: 0.7945\n",
            "Epoch 268/500\n",
            "55/55 [==============================] - 0s 138us/sample - loss: 0.7853\n",
            "Epoch 269/500\n",
            "55/55 [==============================] - 0s 122us/sample - loss: 0.8190\n",
            "Epoch 270/500\n",
            "55/55 [==============================] - 0s 133us/sample - loss: 0.8136\n",
            "Epoch 271/500\n",
            "55/55 [==============================] - 0s 125us/sample - loss: 0.7844\n",
            "Epoch 272/500\n",
            "55/55 [==============================] - 0s 140us/sample - loss: 0.7976\n",
            "Epoch 273/500\n",
            "55/55 [==============================] - 0s 142us/sample - loss: 0.7973\n",
            "Epoch 274/500\n",
            "55/55 [==============================] - 0s 127us/sample - loss: 0.7881\n",
            "Epoch 275/500\n",
            "55/55 [==============================] - 0s 119us/sample - loss: 0.7889\n",
            "Epoch 276/500\n",
            "55/55 [==============================] - 0s 127us/sample - loss: 0.7970\n",
            "Epoch 277/500\n",
            "55/55 [==============================] - 0s 98us/sample - loss: 0.8011\n",
            "Epoch 278/500\n",
            "55/55 [==============================] - 0s 110us/sample - loss: 0.7929\n",
            "Epoch 279/500\n",
            "55/55 [==============================] - 0s 272us/sample - loss: 0.7858\n",
            "Epoch 280/500\n",
            "55/55 [==============================] - 0s 281us/sample - loss: 0.8230\n",
            "Epoch 281/500\n",
            "55/55 [==============================] - 0s 141us/sample - loss: 0.7864\n",
            "Epoch 282/500\n",
            "55/55 [==============================] - 0s 134us/sample - loss: 0.7777\n",
            "Epoch 283/500\n",
            "55/55 [==============================] - 0s 136us/sample - loss: 0.8041\n",
            "Epoch 284/500\n",
            "55/55 [==============================] - 0s 125us/sample - loss: 0.8127\n",
            "Epoch 285/500\n",
            "55/55 [==============================] - 0s 109us/sample - loss: 0.8004\n",
            "Epoch 286/500\n",
            "55/55 [==============================] - 0s 114us/sample - loss: 0.7869\n",
            "Epoch 287/500\n",
            "55/55 [==============================] - 0s 98us/sample - loss: 0.7852\n",
            "Epoch 288/500\n",
            "55/55 [==============================] - 0s 126us/sample - loss: 0.7946\n",
            "Epoch 289/500\n",
            "55/55 [==============================] - 0s 107us/sample - loss: 0.7868\n",
            "Epoch 290/500\n",
            "55/55 [==============================] - 0s 108us/sample - loss: 0.7798\n",
            "Epoch 291/500\n",
            "55/55 [==============================] - 0s 103us/sample - loss: 0.8016\n",
            "Epoch 292/500\n",
            "55/55 [==============================] - 0s 121us/sample - loss: 0.7971\n",
            "Epoch 293/500\n",
            "55/55 [==============================] - 0s 116us/sample - loss: 0.8038\n",
            "Epoch 294/500\n",
            "55/55 [==============================] - 0s 117us/sample - loss: 0.7840\n",
            "Epoch 295/500\n",
            "55/55 [==============================] - 0s 67us/sample - loss: 0.7797\n",
            "Epoch 296/500\n",
            "55/55 [==============================] - 0s 71us/sample - loss: 0.7824\n",
            "Epoch 297/500\n",
            "55/55 [==============================] - 0s 91us/sample - loss: 0.7871\n",
            "Epoch 298/500\n",
            "55/55 [==============================] - 0s 94us/sample - loss: 0.7761\n",
            "Epoch 299/500\n",
            "55/55 [==============================] - 0s 108us/sample - loss: 0.7862\n",
            "Epoch 300/500\n",
            "55/55 [==============================] - 0s 126us/sample - loss: 0.7836\n",
            "Epoch 301/500\n",
            "55/55 [==============================] - 0s 104us/sample - loss: 0.7852\n",
            "Epoch 302/500\n",
            "55/55 [==============================] - 0s 118us/sample - loss: 0.7817\n",
            "Epoch 303/500\n",
            "55/55 [==============================] - 0s 105us/sample - loss: 0.7767\n",
            "Epoch 304/500\n",
            "55/55 [==============================] - 0s 100us/sample - loss: 0.7756\n",
            "Epoch 305/500\n",
            "55/55 [==============================] - 0s 140us/sample - loss: 0.7834\n",
            "Epoch 306/500\n",
            "55/55 [==============================] - 0s 109us/sample - loss: 0.7776\n",
            "Epoch 307/500\n",
            "55/55 [==============================] - 0s 119us/sample - loss: 0.7752\n",
            "Epoch 308/500\n",
            "55/55 [==============================] - 0s 108us/sample - loss: 0.7837\n",
            "Epoch 309/500\n",
            "55/55 [==============================] - 0s 110us/sample - loss: 0.7804\n",
            "Epoch 310/500\n",
            "55/55 [==============================] - 0s 124us/sample - loss: 0.7754\n",
            "Epoch 311/500\n",
            "55/55 [==============================] - 0s 137us/sample - loss: 0.7956\n",
            "Epoch 312/500\n",
            "55/55 [==============================] - 0s 138us/sample - loss: 0.7862\n",
            "Epoch 313/500\n",
            "55/55 [==============================] - 0s 118us/sample - loss: 0.8005\n",
            "Epoch 314/500\n",
            "55/55 [==============================] - 0s 138us/sample - loss: 0.7811\n",
            "Epoch 315/500\n",
            "55/55 [==============================] - 0s 143us/sample - loss: 0.7731\n",
            "Epoch 316/500\n",
            "55/55 [==============================] - 0s 122us/sample - loss: 0.7882\n",
            "Epoch 317/500\n",
            "55/55 [==============================] - 0s 137us/sample - loss: 0.7786\n",
            "Epoch 318/500\n",
            "55/55 [==============================] - 0s 111us/sample - loss: 0.8021\n",
            "Epoch 319/500\n",
            "55/55 [==============================] - 0s 137us/sample - loss: 0.7844\n",
            "Epoch 320/500\n",
            "55/55 [==============================] - 0s 106us/sample - loss: 0.7787\n",
            "Epoch 321/500\n",
            "55/55 [==============================] - 0s 139us/sample - loss: 0.7876\n",
            "Epoch 322/500\n",
            "55/55 [==============================] - 0s 93us/sample - loss: 0.7759\n",
            "Epoch 323/500\n",
            "55/55 [==============================] - 0s 95us/sample - loss: 0.7710\n",
            "Epoch 324/500\n",
            "55/55 [==============================] - 0s 104us/sample - loss: 0.7904\n",
            "Epoch 325/500\n",
            "55/55 [==============================] - 0s 127us/sample - loss: 0.7903\n",
            "Epoch 326/500\n",
            "55/55 [==============================] - 0s 136us/sample - loss: 0.7691\n",
            "Epoch 327/500\n",
            "55/55 [==============================] - 0s 123us/sample - loss: 0.8028\n",
            "Epoch 328/500\n",
            "55/55 [==============================] - 0s 142us/sample - loss: 0.8042\n",
            "Epoch 329/500\n",
            "55/55 [==============================] - 0s 135us/sample - loss: 0.7782\n",
            "Epoch 330/500\n",
            "55/55 [==============================] - 0s 139us/sample - loss: 0.7854\n",
            "Epoch 331/500\n",
            "55/55 [==============================] - 0s 144us/sample - loss: 0.7718\n",
            "Epoch 332/500\n",
            "55/55 [==============================] - 0s 121us/sample - loss: 0.7742\n",
            "Epoch 333/500\n",
            "55/55 [==============================] - 0s 136us/sample - loss: 0.8017\n",
            "Epoch 334/500\n",
            "55/55 [==============================] - 0s 107us/sample - loss: 0.7756\n",
            "Epoch 335/500\n",
            "55/55 [==============================] - 0s 115us/sample - loss: 0.7788\n",
            "Epoch 336/500\n",
            "55/55 [==============================] - 0s 94us/sample - loss: 0.8155\n",
            "Epoch 337/500\n",
            "55/55 [==============================] - 0s 116us/sample - loss: 0.7892\n",
            "Epoch 338/500\n",
            "55/55 [==============================] - 0s 97us/sample - loss: 0.7859\n",
            "Epoch 339/500\n",
            "55/55 [==============================] - 0s 71us/sample - loss: 0.7961\n",
            "Epoch 340/500\n",
            "55/55 [==============================] - 0s 118us/sample - loss: 0.7707\n",
            "Epoch 341/500\n",
            "55/55 [==============================] - 0s 115us/sample - loss: 0.7869\n",
            "Epoch 342/500\n",
            "55/55 [==============================] - 0s 153us/sample - loss: 0.7929\n",
            "Epoch 343/500\n",
            "55/55 [==============================] - 0s 137us/sample - loss: 0.7749\n",
            "Epoch 344/500\n",
            "55/55 [==============================] - 0s 114us/sample - loss: 0.7793\n",
            "Epoch 345/500\n",
            "55/55 [==============================] - 0s 114us/sample - loss: 0.7824\n",
            "Epoch 346/500\n",
            "55/55 [==============================] - 0s 112us/sample - loss: 0.7719\n",
            "Epoch 347/500\n",
            "55/55 [==============================] - 0s 112us/sample - loss: 0.8043\n",
            "Epoch 348/500\n",
            "55/55 [==============================] - 0s 257us/sample - loss: 0.7761\n",
            "Epoch 349/500\n",
            "55/55 [==============================] - 0s 152us/sample - loss: 0.7875\n",
            "Epoch 350/500\n",
            "55/55 [==============================] - 0s 114us/sample - loss: 0.8018\n",
            "Epoch 351/500\n",
            "55/55 [==============================] - 0s 131us/sample - loss: 0.7755\n",
            "Epoch 352/500\n",
            "55/55 [==============================] - 0s 110us/sample - loss: 0.7736\n",
            "Epoch 353/500\n",
            "55/55 [==============================] - 0s 139us/sample - loss: 0.7906\n",
            "Epoch 354/500\n",
            "55/55 [==============================] - 0s 129us/sample - loss: 0.7733\n",
            "Epoch 355/500\n",
            "55/55 [==============================] - 0s 124us/sample - loss: 0.7733\n",
            "Epoch 356/500\n",
            "55/55 [==============================] - 0s 119us/sample - loss: 0.7720\n",
            "Epoch 357/500\n",
            "55/55 [==============================] - 0s 150us/sample - loss: 0.7781\n",
            "Epoch 358/500\n",
            "55/55 [==============================] - 0s 124us/sample - loss: 0.7742\n",
            "Epoch 359/500\n",
            "55/55 [==============================] - 0s 132us/sample - loss: 0.7753\n",
            "Epoch 360/500\n",
            "55/55 [==============================] - 0s 110us/sample - loss: 0.7717\n",
            "Epoch 361/500\n",
            "55/55 [==============================] - 0s 127us/sample - loss: 0.7710\n",
            "Epoch 362/500\n",
            "55/55 [==============================] - 0s 112us/sample - loss: 0.7779\n",
            "Epoch 363/500\n",
            "55/55 [==============================] - 0s 129us/sample - loss: 0.7712\n",
            "Epoch 364/500\n",
            "55/55 [==============================] - 0s 127us/sample - loss: 0.7711\n",
            "Epoch 365/500\n",
            "55/55 [==============================] - 0s 127us/sample - loss: 0.7793\n",
            "Epoch 366/500\n",
            "55/55 [==============================] - 0s 109us/sample - loss: 0.8080\n",
            "Epoch 367/500\n",
            "55/55 [==============================] - 0s 105us/sample - loss: 0.7703\n",
            "Epoch 368/500\n",
            "55/55 [==============================] - 0s 101us/sample - loss: 0.7857\n",
            "Epoch 369/500\n",
            "55/55 [==============================] - 0s 96us/sample - loss: 0.7732\n",
            "Epoch 370/500\n",
            "55/55 [==============================] - 0s 91us/sample - loss: 0.7739\n",
            "Epoch 371/500\n",
            "55/55 [==============================] - 0s 87us/sample - loss: 0.7809\n",
            "Epoch 372/500\n",
            "55/55 [==============================] - 0s 123us/sample - loss: 0.8131\n",
            "Epoch 373/500\n",
            "55/55 [==============================] - 0s 124us/sample - loss: 0.7783\n",
            "Epoch 374/500\n",
            "55/55 [==============================] - 0s 90us/sample - loss: 0.7704\n",
            "Epoch 375/500\n",
            "55/55 [==============================] - 0s 97us/sample - loss: 0.7840\n",
            "Epoch 376/500\n",
            "55/55 [==============================] - 0s 88us/sample - loss: 0.7938\n",
            "Epoch 377/500\n",
            "55/55 [==============================] - 0s 88us/sample - loss: 0.7884\n",
            "Epoch 378/500\n",
            "55/55 [==============================] - 0s 78us/sample - loss: 0.7726\n",
            "Epoch 379/500\n",
            "55/55 [==============================] - 0s 101us/sample - loss: 0.7805\n",
            "Epoch 380/500\n",
            "55/55 [==============================] - 0s 123us/sample - loss: 0.7720\n",
            "Epoch 381/500\n",
            "55/55 [==============================] - 0s 85us/sample - loss: 0.7753\n",
            "Epoch 382/500\n",
            "55/55 [==============================] - 0s 86us/sample - loss: 0.7726\n",
            "Epoch 383/500\n",
            "55/55 [==============================] - 0s 85us/sample - loss: 0.7955\n",
            "Epoch 384/500\n",
            "55/55 [==============================] - 0s 75us/sample - loss: 0.7740\n",
            "Epoch 385/500\n",
            "55/55 [==============================] - 0s 91us/sample - loss: 0.7728\n",
            "Epoch 386/500\n",
            "55/55 [==============================] - 0s 96us/sample - loss: 0.7816\n",
            "Epoch 387/500\n",
            "55/55 [==============================] - 0s 89us/sample - loss: 0.7704\n",
            "Epoch 388/500\n",
            "55/55 [==============================] - 0s 287us/sample - loss: 0.7868\n",
            "Epoch 389/500\n",
            "55/55 [==============================] - 0s 83us/sample - loss: 0.8190\n",
            "Epoch 390/500\n",
            "55/55 [==============================] - 0s 70us/sample - loss: 0.7709\n",
            "Epoch 391/500\n",
            "55/55 [==============================] - 0s 76us/sample - loss: 0.7732\n",
            "Epoch 392/500\n",
            "55/55 [==============================] - 0s 91us/sample - loss: 0.7714\n",
            "Epoch 393/500\n",
            "55/55 [==============================] - 0s 85us/sample - loss: 0.7719\n",
            "Epoch 394/500\n",
            "55/55 [==============================] - 0s 91us/sample - loss: 0.7733\n",
            "Epoch 395/500\n",
            "55/55 [==============================] - 0s 87us/sample - loss: 0.7723\n",
            "Epoch 396/500\n",
            "55/55 [==============================] - 0s 85us/sample - loss: 0.7711\n",
            "Epoch 397/500\n",
            "55/55 [==============================] - 0s 96us/sample - loss: 0.7728\n",
            "Epoch 398/500\n",
            "55/55 [==============================] - 0s 84us/sample - loss: 0.7711\n",
            "Epoch 399/500\n",
            "55/55 [==============================] - 0s 72us/sample - loss: 0.7737\n",
            "Epoch 400/500\n",
            "55/55 [==============================] - 0s 84us/sample - loss: 0.7927\n",
            "Epoch 401/500\n",
            "55/55 [==============================] - 0s 76us/sample - loss: 0.7776\n",
            "Epoch 402/500\n",
            "55/55 [==============================] - 0s 92us/sample - loss: 0.7821\n",
            "Epoch 403/500\n",
            "55/55 [==============================] - 0s 123us/sample - loss: 0.7725\n",
            "Epoch 404/500\n",
            "55/55 [==============================] - 0s 104us/sample - loss: 0.7818\n",
            "Epoch 405/500\n",
            "55/55 [==============================] - 0s 93us/sample - loss: 0.7763\n",
            "Epoch 406/500\n",
            "55/55 [==============================] - 0s 170us/sample - loss: 0.7712\n",
            "Epoch 407/500\n",
            "55/55 [==============================] - 0s 82us/sample - loss: 0.7784\n",
            "Epoch 408/500\n",
            "55/55 [==============================] - 0s 77us/sample - loss: 0.7920\n",
            "Epoch 409/500\n",
            "55/55 [==============================] - 0s 88us/sample - loss: 0.7716\n",
            "Epoch 410/500\n",
            "55/55 [==============================] - 0s 97us/sample - loss: 0.7728\n",
            "Epoch 411/500\n",
            "55/55 [==============================] - 0s 73us/sample - loss: 0.7733\n",
            "Epoch 412/500\n",
            "55/55 [==============================] - 0s 96us/sample - loss: 0.7928\n",
            "Epoch 413/500\n",
            "55/55 [==============================] - 0s 92us/sample - loss: 0.7748\n",
            "Epoch 414/500\n",
            "55/55 [==============================] - 0s 95us/sample - loss: 0.7744\n",
            "Epoch 415/500\n",
            "55/55 [==============================] - 0s 73us/sample - loss: 0.7841\n",
            "Epoch 416/500\n",
            "55/55 [==============================] - 0s 90us/sample - loss: 0.8019\n",
            "Epoch 417/500\n",
            "55/55 [==============================] - 0s 98us/sample - loss: 0.7811\n",
            "Epoch 418/500\n",
            "55/55 [==============================] - 0s 93us/sample - loss: 0.7763\n",
            "Epoch 419/500\n",
            "55/55 [==============================] - 0s 81us/sample - loss: 0.7836\n",
            "Epoch 420/500\n",
            "55/55 [==============================] - 0s 96us/sample - loss: 0.7699\n",
            "Epoch 421/500\n",
            "55/55 [==============================] - 0s 105us/sample - loss: 0.7733\n",
            "Epoch 422/500\n",
            "55/55 [==============================] - 0s 76us/sample - loss: 0.7854\n",
            "Epoch 423/500\n",
            "55/55 [==============================] - 0s 87us/sample - loss: 0.7722\n",
            "Epoch 424/500\n",
            "55/55 [==============================] - 0s 78us/sample - loss: 0.7701\n",
            "Epoch 425/500\n",
            "55/55 [==============================] - 0s 83us/sample - loss: 0.7754\n",
            "Epoch 426/500\n",
            "55/55 [==============================] - 0s 82us/sample - loss: 0.7771\n",
            "Epoch 427/500\n",
            "55/55 [==============================] - 0s 76us/sample - loss: 0.8191\n",
            "Epoch 428/500\n",
            "55/55 [==============================] - 0s 86us/sample - loss: 0.7768\n",
            "Epoch 429/500\n",
            "55/55 [==============================] - 0s 71us/sample - loss: 0.7979\n",
            "Epoch 430/500\n",
            "55/55 [==============================] - 0s 94us/sample - loss: 0.7959\n",
            "Epoch 431/500\n",
            "55/55 [==============================] - 0s 108us/sample - loss: 0.7788\n",
            "Epoch 432/500\n",
            "55/55 [==============================] - 0s 100us/sample - loss: 0.7747\n",
            "Epoch 433/500\n",
            "55/55 [==============================] - 0s 87us/sample - loss: 0.7725\n",
            "Epoch 434/500\n",
            "55/55 [==============================] - 0s 85us/sample - loss: 0.7752\n",
            "Epoch 435/500\n",
            "55/55 [==============================] - 0s 94us/sample - loss: 0.7727\n",
            "Epoch 436/500\n",
            "55/55 [==============================] - 0s 106us/sample - loss: 0.7739\n",
            "Epoch 437/500\n",
            "55/55 [==============================] - 0s 95us/sample - loss: 0.7739\n",
            "Epoch 438/500\n",
            "55/55 [==============================] - 0s 78us/sample - loss: 0.7714\n",
            "Epoch 439/500\n",
            "55/55 [==============================] - 0s 78us/sample - loss: 0.7838\n",
            "Epoch 440/500\n",
            "55/55 [==============================] - 0s 104us/sample - loss: 0.7714\n",
            "Epoch 441/500\n",
            "55/55 [==============================] - 0s 107us/sample - loss: 0.7781\n",
            "Epoch 442/500\n",
            "55/55 [==============================] - 0s 103us/sample - loss: 0.7771\n",
            "Epoch 443/500\n",
            "55/55 [==============================] - 0s 95us/sample - loss: 0.7903\n",
            "Epoch 444/500\n",
            "55/55 [==============================] - 0s 99us/sample - loss: 0.7838\n",
            "Epoch 445/500\n",
            "55/55 [==============================] - 0s 144us/sample - loss: 0.7721\n",
            "Epoch 446/500\n",
            "55/55 [==============================] - 0s 92us/sample - loss: 0.8026\n",
            "Epoch 447/500\n",
            "55/55 [==============================] - 0s 85us/sample - loss: 0.7666\n",
            "Epoch 448/500\n",
            "55/55 [==============================] - 0s 93us/sample - loss: 0.8132\n",
            "Epoch 449/500\n",
            "55/55 [==============================] - 0s 92us/sample - loss: 0.7977\n",
            "Epoch 450/500\n",
            "55/55 [==============================] - 0s 89us/sample - loss: 0.7839\n",
            "Epoch 451/500\n",
            "55/55 [==============================] - 0s 86us/sample - loss: 0.8334\n",
            "Epoch 452/500\n",
            "55/55 [==============================] - 0s 101us/sample - loss: 0.8050\n",
            "Epoch 453/500\n",
            "55/55 [==============================] - 0s 99us/sample - loss: 0.7975\n",
            "Epoch 454/500\n",
            "55/55 [==============================] - 0s 78us/sample - loss: 0.7861\n",
            "Epoch 455/500\n",
            "55/55 [==============================] - 0s 91us/sample - loss: 0.7716\n",
            "Epoch 456/500\n",
            "55/55 [==============================] - 0s 76us/sample - loss: 0.7872\n",
            "Epoch 457/500\n",
            "55/55 [==============================] - 0s 101us/sample - loss: 0.7849\n",
            "Epoch 458/500\n",
            "55/55 [==============================] - 0s 104us/sample - loss: 0.7797\n",
            "Epoch 459/500\n",
            "55/55 [==============================] - 0s 99us/sample - loss: 0.7708\n",
            "Epoch 460/500\n",
            "55/55 [==============================] - 0s 93us/sample - loss: 0.7740\n",
            "Epoch 461/500\n",
            "55/55 [==============================] - 0s 95us/sample - loss: 0.7884\n",
            "Epoch 462/500\n",
            "55/55 [==============================] - 0s 83us/sample - loss: 0.7762\n",
            "Epoch 463/500\n",
            "55/55 [==============================] - 0s 92us/sample - loss: 0.7706\n",
            "Epoch 464/500\n",
            "55/55 [==============================] - 0s 96us/sample - loss: 0.7772\n",
            "Epoch 465/500\n",
            "55/55 [==============================] - 0s 95us/sample - loss: 0.7803\n",
            "Epoch 466/500\n",
            "55/55 [==============================] - 0s 96us/sample - loss: 0.7743\n",
            "Epoch 467/500\n",
            "55/55 [==============================] - 0s 94us/sample - loss: 0.7946\n",
            "Epoch 468/500\n",
            "55/55 [==============================] - 0s 88us/sample - loss: 0.7683\n",
            "Epoch 469/500\n",
            "55/55 [==============================] - 0s 89us/sample - loss: 0.7965\n",
            "Epoch 470/500\n",
            "55/55 [==============================] - 0s 75us/sample - loss: 0.7731\n",
            "Epoch 471/500\n",
            "55/55 [==============================] - 0s 69us/sample - loss: 0.7906\n",
            "Epoch 472/500\n",
            "55/55 [==============================] - 0s 63us/sample - loss: 0.7901\n",
            "Epoch 473/500\n",
            "55/55 [==============================] - 0s 73us/sample - loss: 0.7782\n",
            "Epoch 474/500\n",
            "55/55 [==============================] - 0s 95us/sample - loss: 0.7851\n",
            "Epoch 475/500\n",
            "55/55 [==============================] - 0s 88us/sample - loss: 0.7714\n",
            "Epoch 476/500\n",
            "55/55 [==============================] - 0s 99us/sample - loss: 0.7796\n",
            "Epoch 477/500\n",
            "55/55 [==============================] - 0s 87us/sample - loss: 0.7812\n",
            "Epoch 478/500\n",
            "55/55 [==============================] - 0s 85us/sample - loss: 0.7714\n",
            "Epoch 479/500\n",
            "55/55 [==============================] - 0s 95us/sample - loss: 0.7745\n",
            "Epoch 480/500\n",
            "55/55 [==============================] - 0s 94us/sample - loss: 0.7716\n",
            "Epoch 481/500\n",
            "55/55 [==============================] - 0s 80us/sample - loss: 0.7787\n",
            "Epoch 482/500\n",
            "55/55 [==============================] - 0s 82us/sample - loss: 0.7757\n",
            "Epoch 483/500\n",
            "55/55 [==============================] - 0s 95us/sample - loss: 0.8161\n",
            "Epoch 484/500\n",
            "55/55 [==============================] - 0s 94us/sample - loss: 0.7719\n",
            "Epoch 485/500\n",
            "55/55 [==============================] - 0s 89us/sample - loss: 0.7847\n",
            "Epoch 486/500\n",
            "55/55 [==============================] - 0s 96us/sample - loss: 0.7829\n",
            "Epoch 487/500\n",
            "55/55 [==============================] - 0s 96us/sample - loss: 0.7862\n",
            "Epoch 488/500\n",
            "55/55 [==============================] - 0s 102us/sample - loss: 0.7964\n",
            "Epoch 489/500\n",
            "55/55 [==============================] - 0s 93us/sample - loss: 0.7645\n",
            "Epoch 490/500\n",
            "55/55 [==============================] - 0s 99us/sample - loss: 0.7929\n",
            "Epoch 491/500\n",
            "55/55 [==============================] - 0s 118us/sample - loss: 0.7843\n",
            "Epoch 492/500\n",
            "55/55 [==============================] - 0s 107us/sample - loss: 0.7745\n",
            "Epoch 493/500\n",
            "55/55 [==============================] - 0s 94us/sample - loss: 0.7888\n",
            "Epoch 494/500\n",
            "55/55 [==============================] - 0s 94us/sample - loss: 0.7689\n",
            "Epoch 495/500\n",
            "55/55 [==============================] - 0s 94us/sample - loss: 0.7802\n",
            "Epoch 496/500\n",
            "55/55 [==============================] - 0s 105us/sample - loss: 0.7819\n",
            "Epoch 497/500\n",
            "55/55 [==============================] - 0s 91us/sample - loss: 0.7753\n",
            "Epoch 498/500\n",
            "55/55 [==============================] - 0s 92us/sample - loss: 0.8208\n",
            "Epoch 499/500\n",
            "55/55 [==============================] - 0s 89us/sample - loss: 0.7780\n",
            "Epoch 500/500\n",
            "55/55 [==============================] - 0s 99us/sample - loss: 0.7834\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GFcIU2-SdCrI"
      },
      "source": [
        "আমরা সামনের জুপিটার/কোলাব নোটবুকে আরো উদাহরণ দেখবো। ঘাবড়াবেন না। "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0-QsNCLD4MJZ"
      },
      "source": [
        "## ট্রেনিং এর সাথে লস কমানোর একটা ছবি \n",
        "\n",
        "বরাবরের মতো ফিট মেথডটার আউটপুট একটা অবজেক্ট এ ফেরত পাঠাচ্ছি। এখানকার অবজেক্টের নাম বলছি হিস্ট্রি, সেটা যে কোন নামেই হতে পারে। এখন এই অবজেক্টকে ঠিকমতো প্লট করলে বোঝা যাবে প্রতিটা ট্রেনিং এর সাথে কিভাবে মডেলের লস কমে আসে। শুরুর দিকের বেশি লস - মানে হচ্ছে আমরা যে তাপমাত্রাকে প্রেডিক্ট করতে চাচ্ছি, তার থেকে ট্রেনিং ডাটাবেজে যে তাপমাত্রা আছে সেটা আসলে বেশি ছিল। বেশি ইপকের সাথে সাথে কমে এসেছে সেই লস এর মাত্রা।\n",
        "\n",
        "আগের বইয়ের মত আমরা এখানে ‘ম্যাটপ্লটলিব’ লাইব্রেরি ব্যবহার করছি ডেটা ভিজুয়ালাইজেশন এর জন্য। ভালোভাবে লক্ষ্য করলেই বোঝা যাবে আমাদের মডেল শুরুতেই কিন্তু কমিয়ে নিয়েছে লসের মাত্রা, তবে মাঝে সেটা একটু কমে গিয়েছিল যার শেষের দিকে সেটা একদম শূন্যের কাছাকাছি চলে গিয়েছে। এর মানে হচ্ছে মডেলটা অনেকটাই ভালো পারফরম্যান্স দেবে।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IeK6BzfbdO6_",
        "outputId": "ad25e6e8-0564-44e7-8a32-c8fee1263661",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel(\"Loss Level\")\n",
        "plt.plot(history.history['loss'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f8efbd80748>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPwlIxZFn+m7+AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LtQGDMob5LOD"
      },
      "source": [
        "## মডেলকে দিয়ে প্রেডিক্ট করাই তাপমাত্রা\n",
        "আমাদের হাতে চলে এলো এমন একটা মডেল যাকে ট্রেইন করা হয়েছে আমাদের ১৫ সেকেন্ডের ঝিঁঝিঁপোকার ডাকের সংখ্যার সাথে তার করেসপন্ডিং তাপমাত্রা। তাহলে তো আমরা একটা অজানা ১৫ সেকেন্ডের ঝিঁঝিঁ পোকার ডাক এর সংখ্যা দিলে মডেল বলে দিতে পারবে ওই মুহূর্তের তাপমাত্রা। ভুল বললাম?\n",
        "\n",
        "আমাদেরকে দেখতে হবে কোন ডাটাটা সেই ৫৫টা রেকর্ড এর মধ্যে নেই।\n",
        "\n",
        "৩৪, মানে  ১৫ সেকেন্ডের ঝিঁঝিঁপোকার ডাকের সংখ্যা = ৩৪\n",
        "\n",
        "এখন প্রেডিক্ট করতে হবে ওই সময়ে তাপমাত্রা কতো ছিলো? পারবোনা?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oxNzL4lS2Gui",
        "outputId": "b9ed3a2c-a92f-4b41-8e75-83e3dab626bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(model.predict([34]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[21.205147]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jApk6tZ1fBg1"
      },
      "source": [
        "এর আসল উত্তর হবে $34 \\times 0.49543811976 + 4.45863851637 = 21.303534$, এর মানে হচ্ছে আমাদের মডেল একদম প্রায় মিলিয়ে দিয়েছে।\n",
        "\n",
        "### আমরা কি করলাম?\n",
        "\n",
        "\n",
        "*   আমরা একটা ডেন্স লেয়ার মডেল তৈরি করেছি। \n",
        "*   আমরা সেটাকে ট্রেইন করেছি ২৭,৫০০ এক্সাম্পল দিয়ে (৫৫ জোড়া ইনপুট, ৫০০ ইপক).\n",
        "\n",
        "আমাদের মডেল ডেন্স লেয়ারে ইন্টারনাল ভ্যারিয়েবল (ওয়েট)গুলোকে সেভাবেই টিউন করেছে যাতে ঠিক তাপমাত্রাটা বলতে পারে যদি কেউ ১৫ সেকেন্ডের ওই সময়ের ঝিঁঝিঁপোকার ডাকের সংখ্যা দিতে পারেন। "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zRrOky5gm20Z"
      },
      "source": [
        "## তাহলে ফর্মুলা কোথায়?\n",
        "\n",
        "আমরা অনেক্ষন ডেন্স লেয়ারের ভেতর ইন্টারনাল ভ্যারিয়েবলের কথা বলেছি। সেটা কি খালি চোখে দেখা যাবে না? অবশ্যই যাবে। \n",
        "\n",
        "কেরাসের `লেয়ারের_নাম.get_weights()` দিলেই চলে আসবে নামপাই অ্যারের লিস্ট হিসেবে।  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kmIkVdkbnZJI",
        "outputId": "dc4c3410-98f5-40bc-9a5f-2b18db1bc6f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "l0.get_weights()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[0.4924147]], dtype=float32), array([4.4630466], dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlcaiDydmx57",
        "colab_type": "text"
      },
      "source": [
        "তাহলে আমাদের ফর্মুলা কি ছিলো?\n",
        "\n",
        "y = 0.49543811976X + 4.45863851637 [y = mX + b]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RSplSnMvnWC-"
      },
      "source": [
        "আমাদের প্রথম ভ্যারিয়েবল m হচ্ছে ~0.4954  আর পরেরটা মানে b হচ্ছে ~4.4586. এর অর্থ হচ্ছে আমাদের মেশিন লার্নিং মডেল ইনপুট ডেটা থেকে ফর্মুলা বের করে ফেলেছে। এটাই চাইছিলাম আমরা। যেহেতু এটা মাত্র একটা লেয়ার, একটা নিউরন - সেকারণে এর আউটকাম এসেছে একটা লাইনের ইকুয়েশনের মতো। "
      ]
    }
  ]
}